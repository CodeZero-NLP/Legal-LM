{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Flow Between Agents\n",
    "\n",
    "The system's data flow is coordinated by the Task Planner Agent:\n",
    "\n",
    "1. Initial Flow: Document → Task Planner → Pre-processor\n",
    "2. Information Extraction: Pre-processor → Context Bank & Task Planner\n",
    "3. Knowledge Gathering: Task Planner → Knowledge Agent → Context Bank & Task Planner\n",
    "4. Compliance Analysis: Task Planner → Compliance Checker (accessing Context Bank)\n",
    "   - If knowledge is insufficient → Knowledge Agent (with the missing fields)\n",
    "   - If knowledge is sufficient → Check compliance for each clause\n",
    "5. Conditional Processing:\n",
    "   - If contradictions: Compliance Checker → Clause Rewriter → Compliance Checker\n",
    "   - If compliant: Compliance Checker → Task Planner\n",
    "6. Summarizing Changes: Task Planner → Post-processor\n",
    "7. Task Completion: Post-processor → Final Output → User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from agents.utils.ollama_client import OllamaClient\n",
    "from agents.utils.api_client import APIClient\n",
    "from typing import Any, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables. Please set it in your .env file.\")\n",
    "\n",
    "print(\"[SETUP] Google API Key loaded successfully.\")\n",
    "\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "if not tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY not found in environment variables. Please set it in your .env file.\")\n",
    "\n",
    "print(\"[SETUP] Tavily API Key loaded successfully.\")\n",
    "\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "if not qdrant_api_key:\n",
    "    raise ValueError(\"QDRANT_API_KEY not found in environment variables. Please set it in your .env file.\")\n",
    "\n",
    "print(\"[SETUP] Qdrant API Key loaded successfully.\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "print(\"[SETUP] Initialized ChatGoogleGenerativeAI model with gemini-2.0-flash\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "print(\"[SETUP] Initialized GoogleGenerativeAIEmbeddings model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM client to be used by tools\n",
    "\n",
    "def _initialize_llm_client(use_ollama: bool, model_name: str) -> Any:\n",
    "    \"\"\"Initialize and return the appropriate LLM client based on settings.\"\"\"\n",
    "    try:\n",
    "        if use_ollama:\n",
    "            print(f\"[SETUP] Initializing Ollama client with model {model_name}\")\n",
    "            return OllamaClient(model_name)\n",
    "        else:\n",
    "            print(f\"[SETUP] Initializing API client with model {model_name}\")\n",
    "            return APIClient(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to initialize LLM client: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Bank Initialization\n",
    "\n",
    "Initialize a shared context bank instance that will be used by all agents to store and retrieve document information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_bank import ContextBank\n",
    "\n",
    "# Initialize a single shared context bank instance\n",
    "context_bank = ContextBank()\n",
    "\n",
    "# This context_bank will be passed to all agents that need to store or retrieve information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Pre Processor Agent\", description=\"Transfer when pre-processing is needed, it helps to format and clean the input data.\"),\n",
    "    create_handoff_tool(agent_name=\"Knowledge Agent\", description=\"Transfer when knowledge is needed, it helps to retrieve knowledge from the web using websearcher.\"),\n",
    "    create_handoff_tool(agent_name=\"Compliance Checker Agent\", description=\"Transfer when compliance checking is needed, it helps to check legal documents for compliance with regulations.\"),\n",
    "    create_handoff_tool(agent_name=\"Post Processor Agent\", description=\"Transfer when post processing is needed, it helps to format and finalize the output.\"),\n",
    "]\n",
    "\n",
    "planner_agent_node = create_react_agent(\n",
    "    model,\n",
    "    planner_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "        You are a Task Planner Agent responsible for coordinating a multi-agent system to analyze legal documents for discrepancies and compliance. Your job is to plan and delegate tasks to specialized agents using the relevant handoff tools and track task completion.\n",
    "\n",
    "        INPUTS:\n",
    "        Problem to solve: use the user prompt\n",
    "        Analyze a legal document for discrepancies and compliance issues.\n",
    "\n",
    "        PLANNED TASKS:\n",
    "\n",
    "        * Preprocess document\n",
    "        * Extract and classify clauses\n",
    "        * Retrieve relevant legal compliance knowledge from the web\n",
    "        * Check clause compliance and legal discrepancies\n",
    "        * Summarize issues and finalize output\n",
    "\n",
    "        AVAILABLE AGENTS:\n",
    "        * Pre Processor Agent: Responsible for pre-processing the document, extracting clauses, and classifying them. Processes the input legal document, adds all the relevant information to the context bank and returns status.\n",
    "        * Knowledge Agent: Responsible for retrieving relevant legal compliance knowledge from the web. Fetches information from the web, adds all the relevant knowledge to a vector DB and returns status.\n",
    "        * Compliance Checker Agent: Responsible for checking the compliance of clauses with legal regulations. Performs the compliance check, returns a list of non-compliant clauses and their details. Also returns status.\n",
    "        * Post Processor Agent: Responsible for summarizing issues and finalizing the output. Formats the final output and returns a summary of the compliance check results. Also returns status.\n",
    "\n",
    "        ACTION: \n",
    "        [IMPORTANT] Status Check - Check status of Preprocessor, Compliance Checker, and Post-Processor agents\n",
    "\n",
    "        If Preprocessor status is not complete, trigger Preprocessor Agent.\n",
    "        Once preprocessing is complete, trigger Knowledge Agent.\n",
    "        After Knowledge Agent retrieves relevant knowledge, trigger Compliance Checker Agent.\n",
    "        After completing clause compliance check, Post Processor Agent is triggered for final output and summary.\n",
    "\n",
    "        Rationale:\n",
    "        [IMPORTANT] Do not stop the workflow at any intermediate step. Always proceed till the final step, i.e., using the Post Processor Agent to generate the final summary for the user.\n",
    "        [EXTREMELY CRITICAL] Each agent’s task is sequentially dependent, ensure no step is skipped in the workflow. Status check ensures no redundant computation and completion of workflow.\n",
    "    \"\"\",\n",
    "    name=\"Planner Agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import spacy\n",
    "import uuid\n",
    "import spacy\n",
    "import json  # Added for pretty printing\n",
    "from agents.compliance_checker import _estimate_jurisdiction\n",
    "\n",
    "# Download the spaCy model if it's not already installed\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load once to avoid redundant loading\n",
    "spacy_ner = spacy.load(\"en_core_web_sm\")\n",
    "print(\"[SETUP] Loaded spaCy NER model en_core_web_sm\")\n",
    "\n",
    "llm_client = _initialize_llm_client(use_ollama=False, model_name=\"gemini-2.0-flash\")\n",
    "\n",
    "# TODO : Update the context bank with the clauses, jurisdiction and the document type/metadata\n",
    "\n",
    "def preprocess_document_tool_implementation(file_path: str, system_prompt) -> dict:\n",
    "    \"\"\"\n",
    "    Consolidated preprocessing tool to be used as a callable function in a multi-agent system.\n",
    "    Extracts text, title, named entities, and clause classifications from a PDF document.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"[PREPROCESS] Starting document preprocessing for: {file_path}\")\n",
    "    # print(f\"[PREPROCESS] Context Bank state at start: {json.dumps(context_bank.get_all(), indent=2)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Step 1: Extract text from PDF\n",
    "    print(\"[PREPROCESS] Step 1: Extracting text from PDF...\")\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "    print(f\"[PREPROCESS] Extracted {len(text)} characters of text\")\n",
    "\n",
    "    print(\"[PREPROCESS] Step 2: Extracting title...\")\n",
    "    title_query = \"what is the title of this document?\" + text[:1000]\n",
    "    title = llm_client.query(title_query)\n",
    "    print(f\"[PREPROCESS] Extracted title: {title}\")\n",
    "\n",
    "    # Step 3: Named Entity Recognition\n",
    "    print(\"[PREPROCESS] Step 3: Performing Named Entity Recognition...\")\n",
    "    doc = spacy_ner(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    print(f\"[PREPROCESS] Extracted {len(entities)} named entities\")\n",
    "    print(f\"[PREPROCESS] Sample entities (first 5): {entities[:5]}\")\n",
    "\n",
    "    # Step 4: Document + Clause Classification via external LLM system\n",
    "    print(\"[PREPROCESS] Step 4: Classifying document and clauses...\")\n",
    "    llm_output = llm_client.query(text, system_prompt)\n",
    "\n",
    "    if isinstance(llm_output, str):\n",
    "        # strip any enclosing backticks (```), whitespace, etc.\n",
    "        llm_output = llm_output.strip().strip(\"```json\").strip(\"`\")\n",
    "        # remove literal \"\\\\n\" sequences that came escaped\n",
    "        llm_output = llm_output.replace(\"\\\\n\", \"\")\n",
    "        try:\n",
    "            llm_output = json.loads(llm_output)\n",
    "            print(f\"[PREPROCESS] Successfully parsed LLM output as JSON\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"[ERROR] Failed to parse LLM output as JSON: {e}\")\n",
    "            print(f\"[ERROR] Raw output: {llm_output[:500]}...\")\n",
    "            raise RuntimeError(\"Failed to parse LLM output as JSON:\\n\" + llm_output)\n",
    "\n",
    "    document_class = llm_output.get(\"CLASS\", \"\")\n",
    "    clause_classes = llm_output.get(\"CLAUSES\", [])\n",
    "    \n",
    "    print(f\"[PREPROCESS] Document class: {document_class}\")\n",
    "    print(f\"[PREPROCESS] Extracted {len(clause_classes)} clauses\")\n",
    "\n",
    "\n",
    "    # Step 5: Jurisdiction Estimation\n",
    "    jurisdiction = _estimate_jurisdiction(text, llm_client)\n",
    "    print(\"[PREPROCESS] Step 5: Estimated jurisdiction as:\", jurisdiction)\n",
    "\n",
    "    # Step 6: Storing in Context Bank\n",
    "    print(\"[PREPROCESS] Step 6: Storing in Context Bank...\")\n",
    "    context_bank.add_document(text, {\n",
    "        \"title\": title,\n",
    "        \"document_type\": document_class,\n",
    "        \"source_file\": file_path\n",
    "    })\n",
    "    context_bank.add_entities(entities)\n",
    "    context_bank.add_clauses(clause_classes)\n",
    "    context_bank.add_jurisdiction(jurisdiction)\n",
    "    print(\"[PREPROCESS] Data successfully stored in Context Bank\")\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*80)\n",
    "    # print(f\"[PREPROCESS] Context Bank state after processing: {json.dumps(context_bank.get_clauses(), indent=2)}\")\n",
    "    # print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Final structured output\n",
    "    return {\n",
    "        \"Document Title\": title,\n",
    "        \"Document Class\": document_class,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def preprocess_document_tool(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a tool function for preprocessing legal documents.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the legal document PDF file.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted information.\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "        You are a Pre-processor Agent, a specialized component in the Legal Document Analysis Framework responsible for extracting critical information from legal documents and storing it in the Context Bank. Your work forms the foundation for all subsequent analysis by other agents in the system.\n",
    "\n",
    "        Core Responsibilities:\n",
    "        Your sole task is to extract and structure information from legal documents, including:\n",
    "        - Classifying the document type and purpose\n",
    "        - Extracting important clauses with their classifications\n",
    "        - Storing all extracted information in a structured format accessible to other agents\n",
    "        - Provide your output as strict JSON\n",
    "        Input:\n",
    "        Legal Contract Document PDF.\n",
    "\n",
    "        Output Format:\n",
    "        {\n",
    "        \"CLASS\": \"Document type classification (e.g., Legal Agreement - Employment Contract)\",\n",
    "        \"CLAUSES\": [\n",
    "            {\"Text\": \"Section 3.1: The term of this agreement shall be...\", \"Category\": \"Term Clause\"},\n",
    "            {\"Text\": \"Section 7.2: All disputes shall be resolved by...\", \"Category\": \"Dispute Resolution\"},\n",
    "            {\"Text\": \"Section 9.5: This agreement shall be governed by...\", \"Category\": \"Governing Law\"}\n",
    "        ]\n",
    "        }\n",
    "\n",
    "        [EXTREMELY CRITICAL] Ensure that the output is strictly in JSON format. Do not include any additional text or explanations. The output must be parsable as JSON.\n",
    "    \"\"\"\n",
    "        \n",
    "        # Call the implementation with the shared context bank\n",
    "    return preprocess_document_tool_implementation(file_path, SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer when pre-processing is completed, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "    preprocess_document_tool\n",
    "]\n",
    "\n",
    "pre_processor_agent_node = create_react_agent(\n",
    "    model,\n",
    "    pre_processor_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "         You are the Pre-processor Agent in a Legal Document Analysis Framework. Your sole function is to extract critical information from legal document PDFs and structure it for other agents.\n",
    "\n",
    "         Core Task:\n",
    "         Using the provided tool, preprocess_document_tool, process an input legal document PDF to:\n",
    "         1.   Extract Full Text:  Get the complete text content.\n",
    "         2.   Classify Document:  Determine the document type (e.g., NDA, Lease, Employment Agreement).\n",
    "         3.   Identify Named Entities (NER):  Extract key entities (Parties, Laws, Dates, Jurisdictions, Monetary Values, etc.).\n",
    "         4.   Extract Key Clauses:  Isolate and classify significant clauses (e.g., Term, Governing Law, Confidentiality).\n",
    "         5.   Store Data:  Structure all extracted information (Text, Class, NER list, Clauses list) as a JSON object in the Context Bank.\n",
    "\n",
    "         Input:  Legal Contract Document PDF.\n",
    "         Output:  JSON object with \"TEXT\", \"CLASS\", \"NER\", and \"CLAUSES\" as keys.\n",
    "\n",
    "         Guidelines: \n",
    "         * Be accurate and comprehensive.\n",
    "         * Preserve original context, especially for clauses.\n",
    "         * Focus on legally significant information and obligations.\n",
    "         * Note any low-confidence classifications.\n",
    "         * [CRITICAL STEP] Return to the Planner Agent to update that the pre-processing is completed.\n",
    "      \"\"\",\n",
    "    name=\"Pre Processor Agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import uuid\n",
    "import traceback\n",
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "# from duckduckgo_search import DDGS\n",
    "from cleantext import clean\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, PointStruct, Distance\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Initialize global components once\n",
    "_qdrant_url = \"https://182dab79-601a-482c-9e3e-89bd4653f656.us-east-1-0.aws.cloud.qdrant.io:6333\"\n",
    "_qdrant_collection = \"text_embeddings\"\n",
    "_qdrant_client = QdrantClient(\n",
    "    url = _qdrant_url, \n",
    "    api_key = qdrant_api_key,\n",
    ")\n",
    "\n",
    "print(f\"List of Qdrant Collections: {_qdrant_client.get_collections()}\")\n",
    "\n",
    "_num_results = 10\n",
    "# _ddgs = DDGS()\n",
    "\n",
    "# Use the GoogleGenerativeAIEmbeddings model instead of OllamaEmbeddings\n",
    "_embeddings_model = embeddings\n",
    "\n",
    "# Optional: create collection if it doesn't exist\n",
    "# Get the embedding dimension from the model by embedding a test string\n",
    "test_embedding = _embeddings_model.embed_query(\"test\")\n",
    "vector_size = len(test_embedding)\n",
    "print(f\"[SETUP] Generated test embedding with dimension: {vector_size}\")\n",
    "\n",
    "try:\n",
    "    print(f\"[SETUP] Creating Qdrant collection with vector size: {vector_size}\")\n",
    "    if not _qdrant_client.collection_exists(_qdrant_collection):\n",
    "        _qdrant_client.create_collection(\n",
    "            collection_name=_qdrant_collection,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[SETUP] Collection '{_qdrant_collection}' already exists; skipping creation.\")\n",
    "\n",
    "    print(f\"[SETUP] Successfully created Qdrant collection '{_qdrant_collection}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to create Qdrant collection. Encountered an exception.\")\n",
    "    print(f\"[ERROR] {type(e).__name__}: {e}\")\n",
    "    traceback.print_exc()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_web_knowledge_tool(query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Searches the web for a legal/policy topic,\n",
    "    scrapes and cleans page content, generates embeddings with Google Gen AI,\n",
    "    stores them in Qdrant, and retrieves top relevant results.\n",
    "    Returns a list of summarized search results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Tavily search\n",
    "    print(f\"[KNOWLEDGE] Searching Tavily for query: {query}\")\n",
    "    # results = list(_ddgs.text(query, max_results=10))\n",
    "\n",
    "    tavily_client = TavilyClient(tavily_api_key)\n",
    "    results = tavily_client.search(query, max_results=10)\n",
    "\n",
    "    # print(f\"[INFO] Tavily results: {results}\")\n",
    "\n",
    "    url_results = results.get(\"results\", [])\n",
    "    \n",
    "    print(f\"[KNOWLEDGE] Found {len(url_results)} results from Tavily search\")\n",
    "\n",
    "\n",
    "    # Step 2: Scrape + clean + embed + store\n",
    "    points_to_upsert = []\n",
    "    successful_urls = 0\n",
    "    for i, result in enumerate(url_results):\n",
    "        try:\n",
    "            url = result[\"url\"]\n",
    "            title = result[\"title\"]\n",
    "            print(f\"[KNOWLEDGE] Processing result {i+1}/{len(url_results)}: {title}\")\n",
    "\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status() # Raise an exception for bad status codes\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \" \".join(p.get_text() for p in paragraphs)\n",
    "            content = \" \".join(content.split()) # Remove extra whitespace\n",
    "            cleaned_content = clean(\n",
    "                content,\n",
    "                fix_unicode=True,\n",
    "                to_ascii=True,\n",
    "                lower=True,\n",
    "                no_line_breaks=True,\n",
    "                lang=\"en\"\n",
    "            )\n",
    "\n",
    "            if not cleaned_content: # Skip if content is empty after cleaning\n",
    "                print(f\"[KNOWLEDGE] No content found or extracted for URL {url}\")\n",
    "                continue\n",
    "\n",
    "            # Generate embeddings using Google Gen AI embeddings model\n",
    "            generated_embeddings = _embeddings_model.embed_query(text=cleaned_content)\n",
    "            point_id = str(uuid.uuid5(uuid.NAMESPACE_URL, url))\n",
    "\n",
    "            points_to_upsert.append(\n",
    "                PointStruct( # Use PointStruct for clarity\n",
    "                    id=point_id,\n",
    "                    vector=generated_embeddings,\n",
    "                    payload={\n",
    "                        \"title\": title,\n",
    "                        \"content\": cleaned_content,\n",
    "                        \"url\": url\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            successful_urls += 1\n",
    "            print(f\"[KNOWLEDGE] Successfully processed URL: {url}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] Failed to fetch URL {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process URL {url}: {e}\") # Catch other potential errors\n",
    "\n",
    "    # Upsert points in batch if any were successfully processed\n",
    "    if points_to_upsert:\n",
    "        print(f\"[KNOWLEDGE] Upserting {len(points_to_upsert)} documents to Qdrant collection\")\n",
    "        try:\n",
    "            _qdrant_client.upsert(\n",
    "                collection_name=_qdrant_collection,\n",
    "                points=points_to_upsert,\n",
    "                wait=True # Optional: wait for operation to complete\n",
    "            )\n",
    "            print(f\"[KNOWLEDGE] Successfully upserted {len(points_to_upsert)} documents to Qdrant\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to upsert to Qdrant: {e}\")\n",
    "    else:\n",
    "        print(f\"[KNOWLEDGE] No documents to upsert to Qdrant\")\n",
    "        \n",
    "    print(f\"[KNOWLEDGE] Retrieved and processed {successful_urls} out of {len(url_results)} URLs\")\n",
    "    return [{\"status\": \"knowledge search completed\", \"total_urls\": len(url_results)}]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knowledge_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when knowledge has been retrieved and pass a summary back to it.\"),\n",
    "    retrieve_web_knowledge_tool\n",
    "]\n",
    "\n",
    "knowledge_agent_node = create_react_agent(\n",
    "    model,\n",
    "    knowledge_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "        You are a Knowledge Retrieval Agent tasked with extracting accurate, up-to-date legal information from official U.S. government sources. You use the retrieve_web_knowledge_tool to search for legal/policy topics, scrape and clean page content, generate embeddings, store them in Qdrant, and retrieve top relevant results. \n",
    "        Your work is crucial for ensuring that the Compliance Checker Agent has access to the most current and relevant legal information.\n",
    "\n",
    "        Use the retrieve_web_knowledge_tool to fetch up-to-date legal data from trusted government sites and perform the following functions:\n",
    "        * Retrieve relevant statutes, regulations, and policies based on a given topic.\n",
    "        * Ensure content is current, authoritative, and clearly summarized.\n",
    "        * Avoid non-official, outdated, or speculative sources.\n",
    "        * Store the retrieved knowledge in a vector database for later use by the Compliance Checker Agent.\n",
    "\n",
    "        Search Query Format:\n",
    "        site:[gov source] \"[TOPIC]\" AND \"[FOCUS]\" AND (\"[KEYWORD1]\" OR \"[KEYWORD2]\") after:[YEAR]\n",
    "\n",
    "        Sources: congress.gov, govinfo.gov, law.cornell.edu, federalregister.gov, ecfr.gov, justice.gov, whitehouse.gov, govinfo.gov\n",
    "\n",
    "        Output Format: Return a simple sentence with the status of the knowledge retrieval.\n",
    "\n",
    "        Guidelines:\n",
    "        * Use only listed government sources.\n",
    "        * Do not fabricate or paraphrase inaccurately.\n",
    "        * If no reliable info is found, say so.\n",
    "        * [CRITICAL STEP] Return to the Planner Agent to update that the knowledge retrieval is completed.\n",
    "    \"\"\",\n",
    "    name=\"Knowledge Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from agents.compliance_checker import check_legal_compliance\n",
    "from context_bank import ContextBank\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from agents.utils.websearcher import WebContentRetriever\n",
    "\n",
    "\n",
    "# TODO : Add a function TOOL to check the relevance of knowledge fetched from the vector database\n",
    "@tool\n",
    "def check_knowledge_relevance_tool() -> List[str]:\n",
    "    \"\"\"\n",
    "    Tool to check the relevance of knowledge fetched from the vector database.\n",
    "    This tool will return a list of topics missing from the vector database. This list will be used by the knowledge agent to fetch the missing knowledge.\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant topics that are needed for complaiance checking but are not present in the vector database.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # fetch all the information from the context bank\n",
    "    context_data = context_bank.get_all()\n",
    "\n",
    "    highlights_system_prompt = \"\"\"\n",
    "        Use the input context data to figure out all the important topics of the legal document. These topics will be necessary to perform a compliance check on the legal document.\n",
    "        These highlights will be needed for compliance checking and will be used to check the relevance of the knowledge fetched from the vector database.\n",
    "        OUTPUT: Only return a comma separated list of important topics from the input context data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the context data to a string format that can be used by the LLM\n",
    "    context_data_str = json.dumps(context_data, indent=2)\n",
    "\n",
    "    highlights_prompt = f\"\"\"{highlights_system_prompt} : context_data = {context_data_str}\"\"\"\n",
    "\n",
    "    highlights = llm_client.query(highlights_prompt)\n",
    "    print(f\"[KNOWLEDGE] Highlights: {highlights}\")\n",
    "\n",
    "    # TODO : Implement this logic to use the context data highlights along with the knowledge from the vector database to check relevance\n",
    "\n",
    "    relevance_system_prompt = \"\"\"\n",
    "        Using the input highlights of the legal document and the knowledge fetched from the vector database, check if the knowledge is relevant to the compliance checking of the legal document.\n",
    "        If the knowledge is relevant, return an empty list. If the knowledge is not relevant, return a list of topics that are missing from the knowledge fetched from the vector database.\n",
    "        The missing topics would be important for compliance checkeing and will be used to fetch missing knowledge from the web using the knowledge agent's websearcher tool.\n",
    "        OUTPUT: Only return a comma separated python list of topics missing from the knowledge fetched from the vector database when compared with the highlights of the legal document.\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"Retrieve all relevant legal knowledge for compliance checking.\"\n",
    "\n",
    "    jurisdiction = context_bank.get_jurisdiction()\n",
    "  \n",
    "    doc_meta_from_bank = context_bank.get_document()\n",
    "    # Extract the document_type from the retrieved metadata\n",
    "    # Provide a default value if the key is missing\n",
    "    document_type = doc_meta_from_bank.get(\"document_type\", \"Unknown Document Type\") \n",
    "\n",
    "    knowledge_data = get_knowledge_from_vector_db(query, jurisdiction, document_type)\n",
    "\n",
    "\n",
    "    missing_topics = []\n",
    "\n",
    "    relevance_prompt = f\"\"\"{relevance_system_prompt}  legal document highlights : {highlights} and knowledge collected so far, fetched from the vector DB : {knowledge_data}\"\"\"\n",
    "\n",
    "    print(f\"[KNOWLEDGE] Performing relevance check\")\n",
    "\n",
    "    missing_topics = llm_client.query(relevance_prompt)\n",
    "\n",
    "    print(f\"[KNOWLEDGE] Missing Topics: {missing_topics}\")\n",
    "\n",
    "    return missing_topics\n",
    "\n",
    "\n",
    "# Create an instance of WebContentRetriever to use for querying the vector database\n",
    "def get_knowledge_from_vector_db(query, jurisdiction, document_type: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieves legal knowledge from the vector database based on the query,\n",
    "    jurisdiction, and knowledge type.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string\n",
    "        jurisdiction: The legal jurisdiction (default: \"US\")\n",
    "        \n",
    "    Returns:\n",
    "        List of relevant knowledge items with title, content, URL, and relevance score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Refine the query with jurisdiction and knowledge type for better results\n",
    "    refined_query = f\"{query} jurisdiction:{jurisdiction} document_type:{document_type}\"\n",
    "    print(f\"[KNOWLEDGE RETRIEVAL] Refined query: {refined_query}\")\n",
    "\n",
    "    try:\n",
    "        print(\"[KNOWLEDGE RETRIEVAL] Querying vector database...\")\n",
    "        query_vec = _embeddings_model.embed_query(text=refined_query)\n",
    "        search_results = _qdrant_client.search(\n",
    "            collection_name=_qdrant_collection,\n",
    "            query_vector=query_vec,\n",
    "            with_payload=True,\n",
    "            limit=_num_results\n",
    "        ) # search returns Hit objects directly\n",
    "\n",
    "        print(f\"[KNOWLEDGE RETRIEVAL] Retrieved {len(search_results)} search results\")\n",
    "\n",
    "        results = [\n",
    "            {\n",
    "                \"title\": result.payload[\"title\"],\n",
    "                \"content\": result.payload[\"content\"][:400] + \"...\",\n",
    "                \"url\": result.payload[\"url\"],\n",
    "                \"relevance\": result.score\n",
    "            }\n",
    "            for result in search_results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to retrieve knowledge from vector DB: {e}\")\n",
    "        return [] # Return empty list on search failure\n",
    "    \n",
    "    print(f\"[KNOWLEDGE RETRIEVAL] Completed with {len(results)} knowledge items\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Create a tool that the ComplianceCheckerAgent can use\n",
    "\n",
    "@tool\n",
    "def compliance_check_tool() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Tool for checking legal document clauses for compliance issues.\n",
    "    Performs the following tasks:\n",
    "    Fetches all legal clauses from the context bank\n",
    "    Retrieves relevant legal laws and regulations from the vector database\n",
    "    Performs compliance checks on the clauses using the gathered knowledge\n",
    "    Detects compliance issues: statutory, precedent-based, internal.\n",
    "    Ensures internal consistency across clauses\n",
    "    Identifies legal risks and their implications\n",
    "    Provides structured legal reasoning and confidence scores\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        List of non-compliant clauses with detailed analysis\n",
    "    \"\"\"\n",
    "\n",
    "    # Adding a delay here to avoid hitting API rate limits\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Get clauses from context bank\n",
    "    clauses = context_bank.get_clauses()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"[COMPLIANCE CHECKER] Starting compliance check for {len(clauses)} clauses\")\n",
    "    # print(f\"[COMPLIANCE CHECKER] Context Bank state at start: {json.dumps(context_bank.get_all(), indent=2)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(f\"The list of clauses is: {clauses}\")\n",
    "\n",
    "\n",
    "    query = \"Retrieve all relevant legal knowledge for compliance checking.\"\n",
    "\n",
    "    jurisdiction = context_bank.get_jurisdiction()\n",
    "  \n",
    "    doc_meta_from_bank = context_bank.get_document()\n",
    "    # Extract the document_type from the retrieved metadata\n",
    "    # Provide a default value if the key is missing\n",
    "    document_type = doc_meta_from_bank.get(\"document_type\", \"Unknown Document Type\") \n",
    "    \n",
    "    print(f\"[COMPLIANCE CHECKER] Using jurisdiction: {jurisdiction}\")\n",
    "    print(f\"[COMPLIANCE CHECKER] Using document type: {document_type}\")\n",
    "\n",
    "    # Create a knowledge retrieval adapter that mimics a knowledge agent\n",
    "    # but actually uses the vector DB directly\n",
    "    print(\"[COMPLIANCE CHECKER] Retrieving knowledge from vector DB...\")\n",
    "    knowledge_data = get_knowledge_from_vector_db(query, jurisdiction, document_type)\n",
    "    print(f\"[COMPLIANCE CHECKER] Retrieved {len(knowledge_data)} knowledge items\")\n",
    "    \n",
    "    print(\"[COMPLIANCE CHECKER] Checking legal compliance...\")\n",
    "    results = check_legal_compliance(\n",
    "        context_bank=context_bank,\n",
    "        knowledge_from_vector_db=knowledge_data,\n",
    "        use_ollama=False,\n",
    "        # model_name=\"llama3.1:latest\",\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        min_confidence=0.75\n",
    "    )\n",
    "    \n",
    "    print(f\"[COMPLIANCE CHECKER] Compliance check completed with {len(results)} results\")\n",
    "    # print(\"\\n\" + \"=\"*80)\n",
    "    # print(f\"[COMPLIANCE CHECKER] Context Bank state after check: {json.dumps(context_bank.get_all(), indent=2)}\")\n",
    "    # print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add analysis of the compliance check results to the context bank\n",
    "\n",
    "\n",
    "compliance_checker_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Knowledge Agent\", description=\"Transfer to Knowledge Agent if more knowledge is needed, it helps to retrieve knowledge from the web using websearcher.\"),\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when compliance checking is completed and all clauses are found to be compliant, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "    compliance_check_tool,\n",
    "    check_knowledge_relevance_tool\n",
    "]\n",
    "\n",
    "compliance_checker_agent_node = create_react_agent(\n",
    "    model,\n",
    "    compliance_checker_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "      You are the Compliance Checker Agent, responsible for analyzing a list of extracted legal clauses to identify contradictions, ensure statutory compliance, and assess contractual consistency under U.S. law.\n",
    "\n",
    "      You will use the check_knowledge_relevance_tool to check the relevance of the knowledge fetched from the vector database and to identify any missing topics that are needed for compliance checking.\n",
    "      If the tool returns a list of missing topics, you will pass the list of missing topics to the Knowledge Agent to fetch the missing knowledge from the web using the websearcher tool. Do not perform this step more than once.\n",
    "      If the tool returns an empty list, you will proceed with the compliance check using the compliance_check_tool.\n",
    "\n",
    "      The check_knowledge_relevance_tool:\n",
    "      * Fetches all the data from the context bank\n",
    "      * Finds the highlights of the legal document from that data\n",
    "      * Checks the relevance of the knowledge fetched from the vector database against the highlights of the legal document\n",
    "      * Returns a list of missing topics, knowledge for which needs to be fetched from the web using the Knowledge Agent\n",
    "\n",
    "      [CRITICAL] After this is done, use the compliance_check_tool.\n",
    "      You will use the compliance_check_tool to fetch all the **legal clauses from the context bank** and **relevant laws and regulations retrieved from the vector database** to perform a compliance check for the clauses against the legal knowledge.\n",
    "      Your task is to ensure that the clauses are compliant with federal, state, and city laws, and to identify any legal risks or implications associated with non-compliance.\n",
    "      Your work is crucial for ensuring that the legal document is compliant with all relevant laws and regulations.\n",
    "\n",
    "      Use the compliance_check_tool to perform the following primary functions:\n",
    "      * Fetch all legal clauses from the context bank\n",
    "      * Retrieve relevant legal laws and regulations from the vector database\n",
    "      * Perform compliance checks on the clauses using the gathered knowledge\n",
    "      * Detect compliance issues: statutory, precedent-based, internal.\n",
    "      * Ensure internal consistency across clauses\n",
    "      * Identify legal risks and their implications\n",
    "      * Provide structured legal reasoning and confidence scores\n",
    "\n",
    "      Output:\n",
    "      1. Contradiction Report\n",
    "      {\n",
    "        \"has_contradiction\": true|false,\n",
    "        \"contradiction_type\": \"statutory|precedent|internal\",\n",
    "        \"severity\": \"high|medium|low\",\n",
    "        \"description\": \"...\",\n",
    "        \"source_clause\": { \"id\": \"...\", \"text\": \"...\" },\n",
    "        \"reference\": { \"type\": \"...\", \"id\": \"...\", \"text\": \"...\" }\n",
    "      }\n",
    "      2. Reasoning & Analysis\n",
    "      {\n",
    "        \"analysis_steps\": [\"Step 1...\", \"Step 2...\", \"Step 3...\"],\n",
    "        \"confidence_score\": 0.0–1.0,\n",
    "        \"supporting_references\": [{ \"type\": \"statute\", \"id\": \"...\", \"relevance\": \"...\" }]\n",
    "      }\n",
    "      3. Legal Implications\n",
    "      {\n",
    "        \"implications\": [\n",
    "          {\n",
    "            \"description\": \"...\",\n",
    "            \"severity\": \"high|medium|low\",\n",
    "            \"affected_parties\": [\"...\"],\n",
    "            \"risk_areas\": [\"...\"]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "\n",
    "\n",
    "      [CRITICAL STEP] Decision Flow:\n",
    "      If there are topics missing according to the check_knowledge_relevance_tool, call the Knowledge Agent with the missing topics to retrieve information on them.\n",
    "      If compliance check is complete, return to the Planner Agent for post processing where the whole process is summarized by the Post Processor Agent.\n",
    "\n",
    "      Guidelines:\n",
    "      * Use only validated legal sources\n",
    "      * No fabrication or assumptions\n",
    "      * Flag unclear issues and recommend human review when needed\n",
    "      * Consider jurisdictional scope and maintain objectivity\n",
    "      * [CRITICAL STEP] Return to the Planner Agent to update that the compliance check is completed.\n",
    "    \"\"\",\n",
    "    name=\"Compliance Checker Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO : Add the other tools for Clause Rewriter Agent -\n",
    "\n",
    "# clause_rewriter_agent_tools = [\n",
    "#     create_handoff_tool(agent_name=\"Compliance Checker Agent\", description=\"Transfer to Compliance Checker Agent after a non-compliant clause has been rewritten, it helps to check the compliance of the rewritten clause.\"),\n",
    "# ]\n",
    "\n",
    "# clause_rewriter_agent_node = create_react_agent(\n",
    "#     model,\n",
    "#     clause_rewriter_agent_tools,\n",
    "#     prompt=\"\"\"\n",
    "#     You are the Clause Rewriter Agent, tasked with revising legal clauses flagged as non-compliant, contradictory, or unclear by the Compliance Checker Agent. Your goal is to ensure legal compliance while preserving the original intent.\n",
    "\n",
    "# Responsibilities:\n",
    "# Rewrite clauses to resolve statutory, precedent-based, or internal contradictions\n",
    "# Ensure clarity, enforceability, and alignment with U.S. law\n",
    "# Maintain intent and context of original clause\n",
    "# Signal if more legal context is required (route to Knowledge Agent)\n",
    "\n",
    "# Input Format:\n",
    "# {\n",
    "#   \"original_clause\": {\n",
    "#     \"id\": \"clause_id\",\n",
    "#     \"text\": \"original text\"\n",
    "#   },\n",
    "#   \"issue\": {\n",
    "#     \"description\": \"reason for non-compliance\",\n",
    "#     \"contradiction_type\": \"statutory|precedent|internal\",\n",
    "#     \"reference\": {\n",
    "#       \"type\": \"statute|precedent|clause\",\n",
    "#       \"text\": \"reference text\",\n",
    "#       \"source_link\": \"optional\"\n",
    "#     }\n",
    "#   },\n",
    "#   \"context_info\": {\n",
    "#     \"document_title\": \"Title\",\n",
    "#     \"named_entities\": [...],\n",
    "#     \"document_class\": \"e.g., NDA, Lease\"\n",
    "#   }\n",
    "# }\n",
    "# Output Format:\n",
    "# {\n",
    "#   \"clause_id\": \"clause_id\",\n",
    "#   \"rewritten_clause\": \"Compliant version of the clause\",\n",
    "#   \"justification\": \"How it resolves the issue and aligns with legal standards\"\n",
    "# }\n",
    "# Guidelines:\n",
    "# Be concise, precise, and legally sound\n",
    "# Do not fabricate or generalize\n",
    "# Flag insufficient context when needed\n",
    "\n",
    "#     \"\"\",\n",
    "#     name=\"Clause Rewriter Agent\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add the other tools for Post-Processor Agent - Process Summarizer, Context Bank getter\n",
    "\n",
    "post_processor_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when post-processing is completed, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "]\n",
    "\n",
    "post_processor_agent_node = create_react_agent(\n",
    "    model,\n",
    "    post_processor_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "        You are the Post-processor Agent, responsible for generating final, human-readable outputs after a legal document passes all compliance checks.\n",
    "\n",
    "        Input:\n",
    "        Context Bank (document metadata, clause info)\n",
    "        Compliance Checker outputs (reasoning, implications)\n",
    "        History of tasks done\n",
    "        Tools: Summarizer\n",
    "        Outputs (via Process Summarizer):\n",
    "        Contract Summary – Overview of the document\n",
    "        Changes – Highlighted clause modifications\n",
    "        Risks Averted – Legal issues resolved\n",
    "        References – Cited statutes and precedents\n",
    "\n",
    "        Guidelines:\n",
    "        * Be clear, concise, and legally accurate\n",
    "        * Avoid jargon or speculation\n",
    "        * Tailor for legal and business audiences\n",
    "        * [CRITICAL STEP] Return the summary to the Planner Agent and update that the post processing is completed.\n",
    "\"\"\",\n",
    "    name=\"Post Processor Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = InMemorySaver()\n",
    "workflow = create_swarm(\n",
    "    [planner_agent_node, pre_processor_agent_node, knowledge_agent_node, compliance_checker_agent_node, post_processor_agent_node],\n",
    "    default_active_agent=\"Planner Agent\"\n",
    ")\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[WORKFLOW] Multi-agent swarm initialized with the following agents:\")\n",
    "print(\"  - Planner Agent\")\n",
    "print(\"  - Pre Processor Agent\")\n",
    "print(\"  - Knowledge Agent\")\n",
    "print(\"  - Compliance Checker Agent\")\n",
    "print(\"  - Post Processor Agent\")\n",
    "print(\"[WORKFLOW] Default active agent: Planner Agent\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "turn_1 = app.invoke(\n",
    "    {\"messages\": \n",
    "        [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"\"\"You are given a file path the document which you must preprocess to extract clauses. Once the clauses are extracted, fetch all relevant knowledge related to it. Based on the collected knowledge, you should check for compliance of these clauses. Explain the non-compliant clauses, suggest changes and summarize the results for the User. FILE PATH OF DOCUMENT: \\\"./Original and Modified/modified_UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2.pdf\\\" \",\n",
    "            \"\"\"\n",
    "        }]\n",
    "    },\n",
    "    config,\n",
    ")\n",
    "print(turn_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
