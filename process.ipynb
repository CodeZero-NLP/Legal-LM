{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Flow Between Agents\n",
    "\n",
    "The system's data flow is coordinated by the Task Planner Agent:\n",
    "\n",
    "1. Initial Flow: Document → Task Planner → Pre-processor\n",
    "2. Information Extraction: Pre-processor → Context Bank & Task Planner\n",
    "3. Knowledge Gathering: Task Planner → Knowledge Agent → Context Bank & Task Planner\n",
    "4. Compliance Analysis: Task Planner → Compliance Checker (accessing Context Bank)\n",
    "   - If knowledge is insufficient → Knowledge Agent (with the missing fields)\n",
    "   - If knowledge is sufficient → Check compliance for each clause\n",
    "5. Conditional Processing:\n",
    "   - If contradictions: Compliance Checker → Clause Rewriter → Compliance Checker\n",
    "   - If compliant: Compliance Checker → Task Planner\n",
    "6. Summarizing Changes: Task Planner → Post-processor\n",
    "7. Task Completion: Post-processor → Final Output → User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(\n",
    "#     model=\"models/embedding-001\",\n",
    "#     google_api_key=google_api_key\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Bank Initialization\n",
    "\n",
    "Initialize a shared context bank instance that will be used by all agents to store and retrieve document information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_bank import ContextBank\n",
    "\n",
    "# Initialize a single shared context bank instance\n",
    "context_bank = ContextBank()\n",
    "\n",
    "# This context_bank will be passed to all agents that need to store or retrieve information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"pre_processor_agent\", description=\"Transfer when pre-processing is needed, it helps to format and clean the input data.\"),\n",
    "    create_handoff_tool(agent_name=\"knowledge_agent\", description=\"Transfer when knowledge is needed, it helps to retrieve knowledge from the web using websearcher.\"),\n",
    "    create_handoff_tool(agent_name=\"Compliance Checker Agent\", description=\"Transfer when compliance checking is needed, it helps to check legal documents for compliance with regulations.\"),\n",
    "    create_handoff_tool(agent_name=\"Post Processor Agent\", description=\"Transfer when post processing is needed, it helps to format and finalize the output.\"),\n",
    "]\n",
    "\n",
    "planner_agent_node = create_react_agent(\n",
    "    model,\n",
    "    planner_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "        You are a Task Planner Agent responsible for coordinating a multi-agent system to analyze legal documents for discrepancies and compliance. Your job is to plan and delegate tasks to specialized agents, track task completion, and dynamically adapt the plan based on the current system state.\n",
    "\n",
    "        You are aware of the capabilities of the agents and can query or instruct them based on the task at hand. After every task execution, you should validate whether the task was completed successfully. If a task fails or the output is insufficient, you should modify the workflow, reassign the task, or create additional subtasks.\n",
    "\n",
    "        YOu have access to the following agents/tools:\n",
    "        - Knowledge Agent: Transfer when knowledge is needed, it helps to retrieve knowledge from the web using websearcher.\n",
    "\n",
    "        YOU ARE SUPPOSED TO PLAN WHAT AGENTS AND TOOLS TO CALL AND IN WHAT ORDER.\n",
    "    \"\"\",\n",
    "    name=\"Planner Agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import spacy\n",
    "import uuid\n",
    "\n",
    "# Load once to avoid redundant loading\n",
    "_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_document_tool_implementation(file_path: str, context_bank, system_prompt) -> dict:\n",
    "    \"\"\"\n",
    "    Consolidated preprocessing tool to be used as a callable function in a multi-agent system.\n",
    "    Extracts text, title, named entities, and clause classifications from a PDF document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Extract text from PDF\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "\n",
    "    # Step 2: Title Extraction (from first 10 lines)\n",
    "    def extract_title(text: str) -> str:\n",
    "        lines = text.split(\"\\n\")\n",
    "        candidates = []\n",
    "        for i, line in enumerate(lines[:10]):\n",
    "            clean_line = line.strip()\n",
    "            if not clean_line or len(clean_line) < 5:\n",
    "                continue\n",
    "            score = 0\n",
    "            if re.match(r\"^(CONTRACT|AGREEMENT|PETITION|NOTICE|ORDER|BILL|ACT|STATUTE)\\b\", clean_line, re.IGNORECASE):\n",
    "                score += 5\n",
    "            if re.match(r\"^[A-Z\\s\\-]{5,}$\", clean_line):\n",
    "                score += 2\n",
    "            if \"**\" in clean_line or clean_line.center(80) == clean_line:\n",
    "                score += 1\n",
    "            candidates.append((clean_line, score))\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[0][0] if candidates else \"Unknown Title\"\n",
    "\n",
    "    title = extract_title(text)\n",
    "\n",
    "    # Step 3: Named Entity Recognition\n",
    "    doc = _nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Step 4: Document + Clause Classification via external LLM system\n",
    "    llm_output = system_prompt.process_document(text)\n",
    "    document_class = llm_output.get(\"CLASS\", \"\")\n",
    "    clause_classes = llm_output.get(\"CLAUSES\", [])\n",
    "\n",
    "    # Step 5: Context Banking\n",
    "    document_id = str(uuid.uuid4())\n",
    "    context_bank.add_document(document_id, text, {\n",
    "        \"title\": title,\n",
    "        \"document_type\": document_class,\n",
    "        \"source_file\": file_path\n",
    "    })\n",
    "    context_bank.add_entities(document_id, entities)\n",
    "    context_bank.add_clauses(document_id, clause_classes)\n",
    "\n",
    "    # Final structured output\n",
    "    return {\n",
    "        \"Document ID\": document_id,\n",
    "        \"Text Extracted\": text,\n",
    "        \"Document Title\": title,\n",
    "        \"Document Class\": document_class,\n",
    "        \"Important Clauses\": {clause[\"Text\"]: clause[\"Category\"] for clause in clause_classes},\n",
    "        \"Named Entities\": entities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def preprocess_document_tool(description: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a tool function for preprocessing legal documents.\n",
    "    \n",
    "    Args:\n",
    "        description: A description of what the tool does\n",
    "        \n",
    "    Returns:\n",
    "        A callable tool function that can preprocess legal documents\n",
    "    \"\"\"\n",
    "\n",
    "    def _preprocess_document(file_path: str) -> dict:\n",
    "        # Use the shared context bank instance\n",
    "        from agents.utils.system_prompt import SystemPrompt\n",
    "        system_prompt = SystemPrompt()\n",
    "        \n",
    "        # Call the implementation with the shared context bank\n",
    "        return preprocess_document_tool_implementation(file_path, context_bank, system_prompt)\n",
    "    \n",
    "    return _preprocess_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer when pre-processing is completed, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "    preprocess_document_tool(description=\"Preprocess legal documents by rewriting clauses, extracting relevant information, and formatting them for analysis.\")\n",
    "]\n",
    "\n",
    "pre_processor_agent_node = create_react_agent(\n",
    "    model,\n",
    "    pre_processor_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "\n",
    "You are a Preprocessor Agent, a specialized component within a multi-agent legal analysis framework. Your role is to analyze legal documents and extract all relevant information into a structured format for downstream agents. You are the first point of contact in the pipeline and must ensure complete, clean, and accurate extraction.\n",
    "\n",
    "Core Responsibilities\n",
    "\n",
    "You are responsible for the following tasks when a legal document is provided:\n",
    "- Extract the complete text from a legal contract PDF.\n",
    "- Infer and extract the title of the document from the top sections.\n",
    "- Classify the document type and its intended purpose using textual understanding.\n",
    "- Identify named entities such as parties, jurisdictions, laws, organizations, etc.\n",
    "- Extract important clauses, preserving their content and assigning a semantic category.\n",
    "- Store all results in a structured, queryable format in the Context Bank.\n",
    "\n",
    "Available Tools (Abstracted as One Function)\n",
    "\n",
    "You have access to a single preprocess_document_tool() that integrates the capabilities of multiple tools:\n",
    "\n",
    "1. PDF Text Extractor\n",
    "   - Reads and parses PDF documents\n",
    "   - Preserves structure across pages\n",
    "   - Supports OCR for scanned files (optional in later versions)\n",
    "\n",
    "2. Title Inference Module\n",
    "   - Analyzes top section lines for key patterns and formatting\n",
    "   - Uses heuristics to determine the most probable title\n",
    "\n",
    "3. Document Classifier\n",
    "   - Identifies the overall category of the document using LLM-based classification\n",
    "   - Supports hierarchical legal types and subtypes\n",
    "\n",
    "4. Named Entity Recognizer (NER)\n",
    "   - Detects and labels relevant entities\n",
    "   - Categories include parties, laws, jurisdictions, deadlines, financials, etc.\n",
    "\n",
    "5. Clause Extractor\n",
    "   - Segments the document into clauses\n",
    "   - Tags clauses by type (e.g., Term, Dispute, Payment, Governing Law)\n",
    "\n",
    "6. Context Storage\n",
    "   - Automatically stores the full document, title, entity list, and clause map\n",
    "   - Associates each element with a unique document ID\n",
    "   - Enables downstream retrieval and reasoning\n",
    "\n",
    "Processing Guidelines\n",
    "\n",
    "- Parse the document thoroughly—do not skip pages, headers, or annexures.\n",
    "- Prioritize precision in classification and completeness in NER.\n",
    "- Extract clauses in full, preserving their intent and scope.\n",
    "- If there is uncertainty in classification or clause type, annotate your result clearly.\n",
    "- Use document structure and formatting cues (e.g., numbering, headings) to identify sections.\n",
    "\n",
    "Input:\n",
    "A legal document in PDF format provided by the user.\n",
    "\n",
    "Output Format:\n",
    "{\n",
    "  \"TEXT\": \"Complete text extracted from the document\",\n",
    "  \"TITLE\": \"Inferred title from document header\",\n",
    "  \"CLASS\": \"Document type classification (e.g., Legal Agreement - Employment Contract)\",\n",
    "  \"NER\": [\n",
    "    {\"Entity\": \"ACME Corporation\", \"Category\": \"Party-Company\"},\n",
    "    {\"Entity\": \"John Smith\", \"Category\": \"Party-Individual\"},\n",
    "    {\"Entity\": \"Americans With Disabilities Act of 1990\", \"Category\": \"Law\"},\n",
    "    {\"Entity\": \"California\", \"Category\": \"Jurisdiction\"}\n",
    "  ],\n",
    "  \"CLAUSES\": [\n",
    "    {\"Text\": \"Section 3.1: The term of this agreement shall be...\", \"Category\": \"Term Clause\"},\n",
    "    {\"Text\": \"Section 7.2: All disputes shall be resolved by...\", \"Category\": \"Dispute Resolution\"},\n",
    "    {\"Text\": \"Section 9.5: This agreement shall be governed by...\", \"Category\": \"Governing Law\"}\n",
    "  ]\n",
    "}\n",
    "    \"\"\",\n",
    "    name=\"Pre Processor Agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "from cleantext import clean\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, PointStruct, Distance\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Initialize global components once\n",
    "_qdrant_url = \"http://localhost:6333\"\n",
    "_qdrant_collection = \"web_content\"\n",
    "_qdrant_client = QdrantClient(url=_qdrant_url)\n",
    "_num_results = 3\n",
    "_embeddings_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "_ddgs = DDGS()\n",
    "# Optional: create collection if it doesn't exist\n",
    "def _ensure_qdrant_collection():\n",
    "    try:\n",
    "        _qdrant_client.get_collection(collection_name=_qdrant_collection)\n",
    "    except Exception: # Assuming specific exception type if known, e.g., from qdrant_client documentation\n",
    "        _qdrant_client.create_collection(\n",
    "            collection_name=_qdrant_collection,\n",
    "            vectors_config=VectorParams(size=4096, distance=Distance.COSINE) # Use Distance enum\n",
    "        )\n",
    "\n",
    "@tool\n",
    "def retrieve_knowledge_tool(query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Searches the web for a legal/policy topic,\n",
    "    scrapes and cleans page content, generates embeddings with Ollama,\n",
    "    stores them in Qdrant, and retrieves top relevant results.\n",
    "    Returns a list of summarized search results.\n",
    "    \"\"\"\n",
    "    _ensure_qdrant_collection()\n",
    "\n",
    "    # Step 1: DuckDuckGo search\n",
    "    results = list(_ddgs.text(query, max_results=10))\n",
    "\n",
    "    # Step 2: Scrape + clean + embed + store\n",
    "    points_to_upsert = []\n",
    "    for result in results:\n",
    "        url = result[\"href\"]\n",
    "        title = result[\"title\"]\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status() # Raise an exception for bad status codes\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \" \".join(p.get_text() for p in paragraphs)\n",
    "            content = \" \".join(content.split()) # Remove extra whitespace\n",
    "            cleaned_content = clean(\n",
    "                content,\n",
    "                fix_unicode=True,\n",
    "                to_ascii=True,\n",
    "                lower=True,\n",
    "                no_line_breaks=True,\n",
    "                lang=\"en\"\n",
    "            )\n",
    "\n",
    "            if not cleaned_content: # Skip if content is empty after cleaning\n",
    "                print(f\"[INFO] No content found or extracted for URL {url}\")\n",
    "                continue\n",
    "\n",
    "            embeddings = _embeddings_model.embed_query(text=cleaned_content)\n",
    "            point_id = str(uuid.uuid5(uuid.NAMESPACE_URL, url))\n",
    "\n",
    "            points_to_upsert.append(\n",
    "                PointStruct( # Use PointStruct for clarity\n",
    "                    id=point_id,\n",
    "                    vector=embeddings,\n",
    "                    payload={\n",
    "                        \"title\": title,\n",
    "                        \"content\": cleaned_content,\n",
    "                        \"url\": url\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[WARN] Failed to fetch URL {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to process URL {url}: {e}\") # Catch other potential errors\n",
    "\n",
    "    # Upsert points in batch if any were successfully processed\n",
    "    if points_to_upsert:\n",
    "        _qdrant_client.upsert(\n",
    "            collection_name=_qdrant_collection,\n",
    "            points=points_to_upsert,\n",
    "            wait=True # Optional: wait for operation to complete\n",
    "        )\n",
    "\n",
    "    # Step 3: Search in Qdrant using query embedding\n",
    "    try:\n",
    "        query_vec = _embeddings_model.embed_query(text=query)\n",
    "        search_results = _qdrant_client.search(\n",
    "            collection_name=_qdrant_collection,\n",
    "            query_vector=query_vec,\n",
    "            with_payload=True,\n",
    "            limit=_num_results\n",
    "        ) # search returns Hit objects directly\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"title\": result.payload[\"title\"],\n",
    "                \"content\": result.payload[\"content\"][:400] + \"...\",\n",
    "                \"url\": result.payload[\"url\"],\n",
    "                \"score\": result.score\n",
    "            }\n",
    "            for result in search_results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to search Qdrant: {e}\")\n",
    "        return [] # Return empty list on search failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add the other tools for Knowledge Agent - Web searcher, Context Bank setter and getter\n",
    "\n",
    "knowledge_agent_tools = [\n",
    "    retrieve_knowledge_tool,\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when knowledge has been retrieved and pass a summary back to it.\")\n",
    "]\n",
    "\n",
    "knowledge_agent_node = create_react_agent(\n",
    "    model,\n",
    "    knowledge_agent_tools,\n",
    "    prompt=\n",
    "    \"\"\"\n",
    "      You are a Knowledge Retrieval Agent specializing in identifying, extracting, and summarizing verified, authoritative legal information from official U.S. government sources.\n",
    "\n",
    "  Your responsibilities include:\n",
    " - Finding relevant, up-to-date statutes, regulations, and policies based on a given topic.\n",
    " - Ensuring that all information is accurate, non-fabricated, and clearly organized.\n",
    " - Avoiding non-authoritative, outdated, or speculative sources.\n",
    "\n",
    "  Tools:\n",
    " 1. Web Content Retriever: Fetches real-time legal documents, policy updates, and regulations from trusted U.S. government sources.\n",
    " 2. Context Storage: Stores and organizes retrieved content for future access by other agents or systems, allowing continuity and cross-referencing.\n",
    "\n",
    " Input Format:\n",
    " Use this format for constructing your Search Query:\n",
    "\n",
    " site:congress.gov OR site:govinfo.gov OR site:law.cornell.edu OR site:federalregister.gov OR site:ecfr.gov OR site:justice.gov OR site:whitehouse.gov \"[TOPIC]\" AND \"[FOCUS/CONTEXT]\" AND (\"[KEYWORD 1]\" OR \"[KEYWORD 2]\") after:[YEAR]\n",
    "\n",
    "  Where:\n",
    " - \"TOPIC\": Broad legal topic (e.g., “Data Privacy”, “Environmental Law”, “AI Regulation”)\n",
    " - \"FOCUS/CONTEXT\": Specific angle (e.g., “mobile apps”, “state regulations”, “employment law”)\n",
    " - \"KEYWORDS\": Relevant clauses, sections, or named legal entities (e.g., “Section 230”, “Title VII”)\n",
    " - \"after:YEAR\": Ensures latest updates (e.g., after:2023)\n",
    "\n",
    " Guidelines:\n",
    " - Use only trusted U.S. government domains listed above.\n",
    " - Do not fabricate or paraphrase content inaccurately. Avoid summarizing content you do not fully understand or that lacks a clear source.\n",
    " - If no direct source is found, clearly state that no official information could be retrieved.\n",
    " - Self-check using this question: “Is the summary accurate and completely grounded in verifiable information?” (Self Ask Prompting)\n",
    "\n",
    " Output Format:\n",
    "\n",
    " [Law or Regulation Name] — (Source: {Direct Link to Official Text})  \n",
    " Summary: [1–3 sentence summary explaining what this law/regulation covers and how it affects the specified topic.]\n",
    "\n",
    " Key Provisions:\n",
    " - [Section or clause] — [Explanation or impact]\n",
    "    \"\"\",\n",
    "    name=\"knowledge_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.compliance_checker import check_legal_compliance\n",
    "from context_bank import ContextBank\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from agents.utils.websearcher import WebContentRetriever\n",
    "\n",
    "# Create an instance of WebContentRetriever to use for querying the vector database\n",
    "def get_knowledge_from_vector_db(query, jurisdiction=\"US\"):\n",
    "    \"\"\"\n",
    "    Retrieves legal knowledge from the vector database based on the query,\n",
    "    jurisdiction, and knowledge type.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string\n",
    "        jurisdiction: The legal jurisdiction (default: \"US\")\n",
    "        knowledge_type: Type of knowledge to search for (statutes, regulations, precedents)\n",
    "        \n",
    "    Returns:\n",
    "        List of relevant knowledge items with title, content, URL, and relevance score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Refine the query with jurisdiction and knowledge type for better results\n",
    "    refined_query = f\"{query} jurisdiction:{jurisdiction} type:{knowledge_type}\"\n",
    "    \n",
    "    # TODO : Initialize the WebContentRetriever with the appropriate parameters\n",
    "    try:\n",
    "        # Initialize the WebContentRetriever with the appropriate collection\n",
    "        retriever = WebContentRetriever(\n",
    "            qdrant_url=\"http://localhost:6333\",\n",
    "            num_results=5\n",
    "        )\n",
    "        \n",
    "        # Query the vector database\n",
    "        search_results = retriever.search_in_qdrant(refined_query)\n",
    "        \n",
    "        # Return the search results in a structured format\n",
    "        return [\n",
    "            {\n",
    "                \"title\": result[\"title\"],\n",
    "                \"content\": result[\"content\"],\n",
    "                \"relevance\": result[\"score\"],\n",
    "                \"jurisdiction\": jurisdiction,\n",
    "            }\n",
    "            for result in search_results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to retrieve knowledge from vector DB: {e}\")\n",
    "        # Return empty list on search failure\n",
    "        return []\n",
    "\n",
    "# Create a tool that the ComplianceCheckerAgent can use\n",
    "@tool\n",
    "def compliance_check_tool(clauses, document_metadata):\n",
    "    \"\"\"\n",
    "    Tool for checking legal document clauses for compliance issues.\n",
    "    \n",
    "    Args:\n",
    "        clauses: List of clause dictionaries, each containing 'id' and 'text' keys\n",
    "        document_metadata: Document metadata (jurisdiction, document_type, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        List of non-compliant clauses with detailed analysis\n",
    "    \"\"\"\n",
    "    # Create a knowledge retrieval adapter that mimics a knowledge agent\n",
    "    # but actually uses the vector DB directly\n",
    "    knowledge_data = get_knowledge_from_vector_db(query, jurisdiction)\n",
    "    \n",
    "    return check_legal_compliance(\n",
    "        clauses=clauses,\n",
    "        document_metadata=document_metadata,\n",
    "        context_bank=context_bank,\n",
    "        knowledge_from_vector_db=knowledge_data,\n",
    "        use_ollama=False,\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        min_confidence=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add the other tools for Compliance Checker Agent - Clause Compliance Checker (which contains the Statutory Validator, Precedent Analyzer, Contractual Consistency Engine, Hypergraph Analyzer, Confidence Scorer), Context Bank getter\n",
    "\n",
    "compliance_checker_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Knowledge Agent\", description=\"Transfer to Knowledge Agent if more knowledge is needed, it helps to retrieve knowledge from the web using websearcher.\"),\n",
    "    create_handoff_tool(agent_name=\"Clause Rewriter Agent\", description=\"Transfer to Clause Rewriter Agent if a clause has some discrepancy and rewriting is needed, it helps to rewrite non-compliant clauses in the document.\"),\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when compliance checking is completed and all clauses are found to be compliant, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "    compliance_check_tool(description=\"Checks legal document clauses for compliance issues and returns a structured list of non-compliant clauses.\")\n",
    "]\n",
    "\n",
    "compliance_checker_agent_node = create_react_agent(\n",
    "    model,\n",
    "    compliance_checker_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "    \"\"\",\n",
    "    name=\"Compliance Checker Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add the other tools for Clause Rewriter Agent -\n",
    "\n",
    "clause_rewriter_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Compliance Checker Agent\", description=\"Transfer to Compliance Checker Agent after a non-compliant clause has been rewritten, it helps to check the compliance of the rewritten clause.\"),\n",
    "]\n",
    "\n",
    "clause_rewriter_agent_node = create_react_agent(\n",
    "    model,\n",
    "    clause_rewriter_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "    \"\"\",\n",
    "    name=\"Clause Rewriter Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add the other tools for Post-Processor Agent - Process Summarizer, Context Bank getter\n",
    "\n",
    "post_processor_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when post-processing is completed, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "]\n",
    "\n",
    "post_processor_agent_node = create_react_agent(\n",
    "    model,\n",
    "    post_processor_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "    \"\"\",\n",
    "    name=\"Post Processor Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = InMemorySaver()\n",
    "workflow = create_swarm(\n",
    "    [planner_agent_node, pre_processor_agent_node, knowledge_agent_node, compliance_checker_agent_node, clause_rewriter_agent_node, post_processor_agent_node],\n",
    "    default_active_agent=\"Planner Agent\"\n",
    ")\n",
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "turn_1 = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Ask the Knowledge Agent to retrieve information and add the number of letters in the response.\"}]},\n",
    "    config,\n",
    ")\n",
    "print(turn_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_context_bank_data(document_id: str, data_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Tool for retrieving data from the context bank\n",
    "    \n",
    "    Args:\n",
    "        document_id: The ID of the document to retrieve data for\n",
    "        data_type: The type of data to retrieve (\"document\", \"entities\", \"clauses\", or \"all\")\n",
    "        \n",
    "    Returns:\n",
    "        The requested data from the context bank\n",
    "    \"\"\"\n",
    "    if data_type == \"document\":\n",
    "        return context_bank.get_document(document_id)\n",
    "    elif data_type == \"entities\":\n",
    "        return context_bank.get_entities(document_id)\n",
    "    elif data_type == \"clauses\":\n",
    "        return context_bank.get_clauses(document_id)\n",
    "    elif data_type == \"all\":\n",
    "        return {\n",
    "            \"document\": context_bank.get_document(document_id),\n",
    "            \"entities\": context_bank.get_entities(document_id),\n",
    "            \"clauses\": context_bank.get_clauses(document_id)\n",
    "        }\n",
    "    else:\n",
    "        return {\"error\": f\"Invalid data_type: {data_type}. Must be 'document', 'entities', 'clauses', or 'all'.\"}\n",
    "\n",
    "@tool\n",
    "def set_context_bank_data(document_id: str, data_type: str, data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Tool for setting data in the context bank\n",
    "    \n",
    "    Args:\n",
    "        document_id: The ID of the document to set data for\n",
    "        data_type: The type of data to set (\"document\", \"entities\", \"clauses\")\n",
    "        data: The data to set\n",
    "        \n",
    "    Returns:\n",
    "        Success message\n",
    "    \"\"\"\n",
    "    if data_type == \"document\":\n",
    "        context_bank.add_document(document_id, data.get(\"text\", \"\"), data.get(\"metadata\", {}))\n",
    "    elif data_type == \"entities\":\n",
    "        context_bank.add_entities(document_id, data.get(\"entities\", []))\n",
    "    elif data_type == \"clauses\":\n",
    "        context_bank.add_clauses(document_id, data.get(\"clauses\", []))\n",
    "    else:\n",
    "        return {\"error\": f\"Invalid data_type: {data_type}. Must be 'document', 'entities', or 'clauses'.\"}\n",
    "        \n",
    "    return {\"success\": True, \"message\": f\"Successfully set {data_type} data for document {document_id}\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
