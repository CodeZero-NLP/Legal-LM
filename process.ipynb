{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Flow Between Agents\n",
    "\n",
    "The system's data flow is coordinated by the Task Planner Agent:\n",
    "\n",
    "1. Initial Flow: Document → Task Planner → Pre-processor\n",
    "2. Information Extraction: Pre-processor → Context Bank & Task Planner\n",
    "3. Knowledge Gathering: Task Planner → Knowledge Agent → Context Bank & Task Planner\n",
    "4. Compliance Analysis: Task Planner → Compliance Checker (accessing Context Bank)\n",
    "   - If knowledge is insufficient → Knowledge Agent (with the missing fields)\n",
    "   - If knowledge is sufficient → Check compliance for each clause\n",
    "5. Conditional Processing:\n",
    "   - If contradictions: Compliance Checker → Clause Rewriter → Compliance Checker\n",
    "   - If compliant: Compliance Checker → Task Planner\n",
    "6. Summarizing Changes: Task Planner → Post-processor\n",
    "7. Task Completion: Post-processor → Final Output → User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mandarburande/Projects/Legal-LM/legal-lm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from agents.utils.ollama_client import OllamaClient\n",
    "from agents.utils.api_client import APIClient\n",
    "from typing import Any, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] Tavily API Key loaded successfully.\n",
      "[SETUP] Initialized ChatGoogleGenerativeAI model with gemini-2.0-flash\n",
      "[SETUP] Initialized GoogleGenerativeAIEmbeddings model\n"
     ]
    }
   ],
   "source": [
    "# model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "if not tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY not found in environment variables. Please set it in your .env file.\")\n",
    "\n",
    "print(\"[SETUP] Tavily API Key loaded successfully.\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "print(\"[SETUP] Initialized ChatGoogleGenerativeAI model with gemini-2.0-flash\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "print(\"[SETUP] Initialized GoogleGenerativeAIEmbeddings model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM client to be used by tools\n",
    "\n",
    "def _initialize_llm_client(use_ollama: bool, model_name: str) -> Any:\n",
    "    \"\"\"Initialize and return the appropriate LLM client based on settings.\"\"\"\n",
    "    try:\n",
    "        if use_ollama:\n",
    "            print(f\"[SETUP] Initializing Ollama client with model {model_name}\")\n",
    "            return OllamaClient(model_name)\n",
    "        else:\n",
    "            print(f\"[SETUP] Initializing API client with model {model_name}\")\n",
    "            return APIClient(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to initialize LLM client: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Bank Initialization\n",
    "\n",
    "Initialize a shared context bank instance that will be used by all agents to store and retrieve document information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_bank import ContextBank\n",
    "\n",
    "# Initialize a single shared context bank instance\n",
    "context_bank = ContextBank()\n",
    "\n",
    "# This context_bank will be passed to all agents that need to store or retrieve information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Pre Processor Agent\", description=\"Transfer when pre-processing is needed, it helps to format and clean the input data.\"),\n",
    "    create_handoff_tool(agent_name=\"Knowledge Agent\", description=\"Transfer when knowledge is needed, it helps to retrieve knowledge from the web using websearcher.\"),\n",
    "    create_handoff_tool(agent_name=\"Compliance Checker Agent\", description=\"Transfer when compliance checking is needed, it helps to check legal documents for compliance with regulations.\"),\n",
    "    create_handoff_tool(agent_name=\"Post Processor Agent\", description=\"Transfer when post processing is needed, it helps to format and finalize the output.\"),\n",
    "]\n",
    "\n",
    "planner_agent_node = create_react_agent(\n",
    "    model,\n",
    "    planner_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "        You are a Task Planner Agent responsible for coordinating a multi-agent system to analyze legal documents for discrepancies and compliance. Your job is to plan and delegate tasks to specialized agents using the relevant handoff tools and track task completion.\n",
    "\n",
    "        INPUTS:\n",
    "        Problem to solve: use the user prompt\n",
    "        Analyze a legal document for discrepancies and compliance issues.\n",
    "\n",
    "        PLANNED TASKS:\n",
    "\n",
    "        * Preprocess document\n",
    "        * Extract and classify clauses\n",
    "        * Retrieve relevant legal compliance knowledge from the web\n",
    "        * Check clause compliance and legal discrepancies\n",
    "        * Summarize issues and finalize output\n",
    "\n",
    "        AVAILABLE AGENTS:\n",
    "        * Pre Processor Agent: Responsible for pre-processing the document, extracting clauses, and classifying them. Processes the input legal document, adds all the relevant information to the context bank and returns status.\n",
    "        * Knowledge Agent: Responsible for retrieving relevant legal compliance knowledge from the web. Fetches information from the web, adds all the relevant knowledge to a vector DB and returns status.\n",
    "        * Compliance Checker Agent: Responsible for checking the compliance of clauses with legal regulations. Performs the compliance check, returns a list of non-compliant clauses and their details. Also returns status.\n",
    "        * Post Processor Agent: Responsible for summarizing issues and finalizing the output. Formats the final output and returns a summary of the compliance check results. Also returns status.\n",
    "\n",
    "        ACTION: \n",
    "        [IMPORTANT] Status Check - Check status of Preprocessor, Compliance Checker, and Post-Processor agents\n",
    "\n",
    "        If Preprocessor status is not complete, trigger Preprocessor Agent.\n",
    "        Once preprocessing is complete, trigger Knowledge Agent.\n",
    "        After Knowledge Agent retrieves relevant knowledge, trigger Compliance Checker Agent.\n",
    "        After completing clause compliance check, Post Processor Agent is triggered for final output and summary.\n",
    "\n",
    "        Rationale:\n",
    "        [IMPORTANT] Do not stop the workflow at any intermediate step. Always proceed till the final step, i.e., using the Post Processor Agent to generate the final summary for the user.\n",
    "        [EXTREMELY CRITICAL] Each agent’s task is sequentially dependent, ensure no step is skipped in the workflow. Status check ensures no redundant computation and completion of workflow.\n",
    "    \"\"\",\n",
    "    name=\"Planner Agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "[SETUP] Loaded spaCy NER model en_core_web_sm\n",
      "[SETUP] Initializing API client with model gemini-2.0-flash\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import spacy\n",
    "import uuid\n",
    "import spacy\n",
    "import json  # Added for pretty printing\n",
    "\n",
    "# Download the spaCy model if it's not already installed\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load once to avoid redundant loading\n",
    "spacy_ner = spacy.load(\"en_core_web_sm\")\n",
    "print(\"[SETUP] Loaded spaCy NER model en_core_web_sm\")\n",
    "\n",
    "llm_client = _initialize_llm_client(use_ollama=False, model_name=\"gemini-2.0-flash\")\n",
    "\n",
    "# TODO : Update the context bank with the clauses, jurisdiction and the document type/metadata\n",
    "\n",
    "def preprocess_document_tool_implementation(file_path: str, system_prompt) -> dict:\n",
    "    \"\"\"\n",
    "    Consolidated preprocessing tool to be used as a callable function in a multi-agent system.\n",
    "    Extracts text, title, named entities, and clause classifications from a PDF document.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"[PREPROCESS] Starting document preprocessing for: {file_path}\")\n",
    "    # print(f\"[PREPROCESS] Context Bank state at start: {json.dumps(context_bank.get_all(), indent=2)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Step 1: Extract text from PDF\n",
    "    print(\"[PREPROCESS] Step 1: Extracting text from PDF...\")\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "    print(f\"[PREPROCESS] Extracted {len(text)} characters of text\")\n",
    "\n",
    "    # Step 2: Title Extraction (from first 10 lines)\n",
    "    # print(\"[PREPROCESS] Step 2: Extracting title...\")\n",
    "    # def extract_title(text: str) -> str:\n",
    "    #     lines = text.split(\"\\n\")\n",
    "    #     candidates = []\n",
    "    #     for i, line in enumerate(lines[:10]):\n",
    "    #         clean_line = line.strip()\n",
    "    #         if not clean_line or len(clean_line) < 5:\n",
    "    #             continue\n",
    "    #         score = 0\n",
    "    #         if re.match(r\"^(CONTRACT|AGREEMENT|PETITION|NOTICE|ORDER|BILL|ACT|STATUTE)\\b\", clean_line, re.IGNORECASE):\n",
    "    #             score += 5\n",
    "    #         if re.match(r\"^[A-Z\\s\\-]{5,}$\", clean_line):\n",
    "    #             score += 2\n",
    "    #         if \"**\" in clean_line or clean_line.center(80) == clean_line:\n",
    "    #             score += 1\n",
    "    #         candidates.append((clean_line, score))\n",
    "    #     candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    #     return candidates[0][0] if candidates else \"Unknown Title\"\n",
    "\n",
    "    print(\"[PREPROCESS] Step 2: Extracting title...\")\n",
    "    title_query = \"what is the title of this document?\" + text[:1000]\n",
    "    title = llm_client.query(title_query)\n",
    "    print(f\"[PREPROCESS] Extracted title: {title}\")\n",
    "\n",
    "    # Step 3: Named Entity Recognition\n",
    "    print(\"[PREPROCESS] Step 3: Performing Named Entity Recognition...\")\n",
    "    doc = spacy_ner(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    print(f\"[PREPROCESS] Extracted {len(entities)} named entities\")\n",
    "    print(f\"[PREPROCESS] Sample entities (first 5): {entities[:5]}\")\n",
    "\n",
    "    # Step 4: Document + Clause Classification via external LLM system\n",
    "    print(\"[PREPROCESS] Step 4: Classifying document and clauses...\")\n",
    "    llm_output = llm_client.query(text, system_prompt)\n",
    "\n",
    "    if isinstance(llm_output, str):\n",
    "        # strip any enclosing backticks (```), whitespace, etc.\n",
    "        llm_output = llm_output.strip().strip(\"```json\").strip(\"`\")\n",
    "        # remove literal \"\\\\n\" sequences that came escaped\n",
    "        llm_output = llm_output.replace(\"\\\\n\", \"\")\n",
    "        try:\n",
    "            llm_output = json.loads(llm_output)\n",
    "            print(f\"[PREPROCESS] Successfully parsed LLM output as JSON\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"[ERROR] Failed to parse LLM output as JSON: {e}\")\n",
    "            print(f\"[ERROR] Raw output: {llm_output[:500]}...\")\n",
    "            raise RuntimeError(\"Failed to parse LLM output as JSON:\\n\" + llm_output)\n",
    "\n",
    "    document_class = llm_output.get(\"CLASS\", \"\")\n",
    "    clause_classes = llm_output.get(\"CLAUSES\", [])\n",
    "    \n",
    "    print(f\"[PREPROCESS] Document class: {document_class}\")\n",
    "    print(f\"[PREPROCESS] Extracted {len(clause_classes)} clauses\")\n",
    "\n",
    "    # Step 5: Soring in Context Bank\n",
    "    print(\"[PREPROCESS] Step 5: Storing in Context Bank...\")\n",
    "    context_bank.add_document(text, {\n",
    "        \"title\": title,\n",
    "        \"document_type\": document_class,\n",
    "        \"source_file\": file_path\n",
    "    })\n",
    "    context_bank.add_entities(entities)\n",
    "    context_bank.add_clauses(clause_classes)\n",
    "    print(\"[PREPROCESS] Data successfully stored in Context Bank\")\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*80)\n",
    "    # print(f\"[PREPROCESS] Context Bank state after processing: {json.dumps(context_bank.get_clauses(), indent=2)}\")\n",
    "    # print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Final structured output\n",
    "    return {\n",
    "        \"Document Title\": title,\n",
    "        \"Document Class\": document_class,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def preprocess_document_tool(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a tool function for preprocessing legal documents.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the legal document PDF file.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted information.\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "        You are a Pre-processor Agent, a specialized component in the Legal Document Analysis Framework responsible for extracting critical information from legal documents and storing it in the Context Bank. Your work forms the foundation for all subsequent analysis by other agents in the system.\n",
    "\n",
    "        Core Responsibilities:\n",
    "        Your sole task is to extract and structure information from legal documents, including:\n",
    "        - Classifying the document type and purpose\n",
    "        - Extracting important clauses with their classifications\n",
    "        - Storing all extracted information in a structured format accessible to other agents\n",
    "        - Provide your output as strict JSON\n",
    "        Input:\n",
    "        Legal Contract Document PDF.\n",
    "\n",
    "        Output Format:\n",
    "        {\n",
    "        \"CLASS\": \"Document type classification (e.g., Legal Agreement - Employment Contract)\",\n",
    "        \"CLAUSES\": [\n",
    "            {\"Text\": \"Section 3.1: The term of this agreement shall be...\", \"Category\": \"Term Clause\"},\n",
    "            {\"Text\": \"Section 7.2: All disputes shall be resolved by...\", \"Category\": \"Dispute Resolution\"},\n",
    "            {\"Text\": \"Section 9.5: This agreement shall be governed by...\", \"Category\": \"Governing Law\"}\n",
    "        ]\n",
    "        }\n",
    "\n",
    "        [EXTREMELY CRITICAL] Ensure that the output is strictly in JSON format. Do not include any additional text or explanations. The output must be parsable as JSON.\n",
    "    \"\"\"\n",
    "        \n",
    "        # Call the implementation with the shared context bank\n",
    "    return preprocess_document_tool_implementation(file_path, SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer when pre-processing is completed, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "    preprocess_document_tool\n",
    "]\n",
    "\n",
    "pre_processor_agent_node = create_react_agent(\n",
    "    model,\n",
    "    pre_processor_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "         You are the Pre-processor Agent in a Legal Document Analysis Framework. Your sole function is to extract critical information from legal document PDFs and structure it for other agents.\n",
    "\n",
    "         Core Task:\n",
    "         Using the provided tool, preprocess_document_tool, process an input legal document PDF to:\n",
    "         1.   Extract Full Text:  Get the complete text content.\n",
    "         2.   Classify Document:  Determine the document type (e.g., NDA, Lease, Employment Agreement).\n",
    "         3.   Identify Named Entities (NER):  Extract key entities (Parties, Laws, Dates, Jurisdictions, Monetary Values, etc.).\n",
    "         4.   Extract Key Clauses:  Isolate and classify significant clauses (e.g., Term, Governing Law, Confidentiality).\n",
    "         5.   Store Data:  Structure all extracted information (Text, Class, NER list, Clauses list) as a JSON object in the Context Bank.\n",
    "\n",
    "         Input:  Legal Contract Document PDF.\n",
    "         Output:  JSON object with \"TEXT\", \"CLASS\", \"NER\", and \"CLAUSES\" as keys.\n",
    "\n",
    "         Guidelines: \n",
    "         * Be accurate and comprehensive.\n",
    "         * Preserve original context, especially for clauses.\n",
    "         * Focus on legally significant information and obligations.\n",
    "         * Note any low-confidence classifications.\n",
    "         * [CRITICAL STEP] Return to the Planner Agent to update that the pre-processing is completed.\n",
    "      \"\"\",\n",
    "    name=\"Pre Processor Agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] Generated test embedding with dimension: 768\n",
      "[SETUP] Creating Qdrant collection with vector size: 768\n",
      "[SETUP] Collection 'web_content' already exists; skipping creation.\n",
      "[SETUP] Successfully created Qdrant collection 'web_content'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import uuid\n",
    "import traceback\n",
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "from cleantext import clean\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, PointStruct, Distance\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Initialize global components once\n",
    "_qdrant_url = \"http://localhost:6333\"\n",
    "_qdrant_collection = \"web_content\"\n",
    "_qdrant_client = QdrantClient(\n",
    "    url=_qdrant_url, \n",
    "    prefer_grpc=False\n",
    ")\n",
    "\n",
    "_num_results = 3\n",
    "_ddgs = DDGS()\n",
    "\n",
    "# Use the GoogleGenerativeAIEmbeddings model instead of OllamaEmbeddings\n",
    "_embeddings_model = embeddings\n",
    "\n",
    "# Optional: create collection if it doesn't exist\n",
    "# Get the embedding dimension from the model by embedding a test string\n",
    "test_embedding = _embeddings_model.embed_query(\"test\")\n",
    "vector_size = len(test_embedding)\n",
    "print(f\"[SETUP] Generated test embedding with dimension: {vector_size}\")\n",
    "\n",
    "try:\n",
    "    print(f\"[SETUP] Creating Qdrant collection with vector size: {vector_size}\")\n",
    "    if not _qdrant_client.collection_exists(_qdrant_collection):\n",
    "        _qdrant_client.create_collection(\n",
    "            collection_name=_qdrant_collection,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[SETUP] Collection '{_qdrant_collection}' already exists; skipping creation.\")\n",
    "\n",
    "    print(f\"[SETUP] Successfully created Qdrant collection '{_qdrant_collection}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to create Qdrant collection. Encountered an exception.\")\n",
    "    print(f\"[ERROR] {type(e).__name__}: {e}\")\n",
    "    traceback.print_exc()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_web_knowledge_tool(query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Searches the web for a legal/policy topic,\n",
    "    scrapes and cleans page content, generates embeddings with Google Gen AI,\n",
    "    stores them in Qdrant, and retrieves top relevant results.\n",
    "    Returns a list of summarized search results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Tavily search\n",
    "    print(f\"[KNOWLEDGE] Searching Tavily for query: {query}\")\n",
    "    # results = list(_ddgs.text(query, max_results=10))\n",
    "\n",
    "    tavily_client = TavilyClient(tavily_api_key)\n",
    "    results = tavily_client.search(query, max_results=10)\n",
    "\n",
    "    # print(f\"[INFO] Tavily results: {results}\")\n",
    "\n",
    "    url_results = results.get(\"results\", [])\n",
    "    \n",
    "    print(f\"[KNOWLEDGE] Found {len(url_results)} results from Tavily search\")\n",
    "\n",
    "\n",
    "    # Step 2: Scrape + clean + embed + store\n",
    "    points_to_upsert = []\n",
    "    successful_urls = 0\n",
    "    for i, result in enumerate(url_results):\n",
    "        try:\n",
    "            url = result[\"url\"]\n",
    "            title = result[\"title\"]\n",
    "            print(f\"[KNOWLEDGE] Processing result {i+1}/{len(url_results)}: {title}\")\n",
    "\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status() # Raise an exception for bad status codes\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \" \".join(p.get_text() for p in paragraphs)\n",
    "            content = \" \".join(content.split()) # Remove extra whitespace\n",
    "            cleaned_content = clean(\n",
    "                content,\n",
    "                fix_unicode=True,\n",
    "                to_ascii=True,\n",
    "                lower=True,\n",
    "                no_line_breaks=True,\n",
    "                lang=\"en\"\n",
    "            )\n",
    "\n",
    "            if not cleaned_content: # Skip if content is empty after cleaning\n",
    "                print(f\"[KNOWLEDGE] No content found or extracted for URL {url}\")\n",
    "                continue\n",
    "\n",
    "            # Generate embeddings using Google Gen AI embeddings model\n",
    "            generated_embeddings = _embeddings_model.embed_query(text=cleaned_content)\n",
    "            point_id = str(uuid.uuid5(uuid.NAMESPACE_URL, url))\n",
    "\n",
    "            points_to_upsert.append(\n",
    "                PointStruct( # Use PointStruct for clarity\n",
    "                    id=point_id,\n",
    "                    vector=generated_embeddings,\n",
    "                    payload={\n",
    "                        \"title\": title,\n",
    "                        \"content\": cleaned_content,\n",
    "                        \"url\": url\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            successful_urls += 1\n",
    "            print(f\"[KNOWLEDGE] Successfully processed URL: {url}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] Failed to fetch URL {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process URL {url}: {e}\") # Catch other potential errors\n",
    "\n",
    "    # Upsert points in batch if any were successfully processed\n",
    "    if points_to_upsert:\n",
    "        print(f\"[KNOWLEDGE] Upserting {len(points_to_upsert)} documents to Qdrant collection\")\n",
    "        try:\n",
    "            _qdrant_client.upsert(\n",
    "                collection_name=_qdrant_collection,\n",
    "                points=points_to_upsert,\n",
    "                wait=True # Optional: wait for operation to complete\n",
    "            )\n",
    "            print(f\"[KNOWLEDGE] Successfully upserted {len(points_to_upsert)} documents to Qdrant\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to upsert to Qdrant: {e}\")\n",
    "    else:\n",
    "        print(f\"[KNOWLEDGE] No documents to upsert to Qdrant\")\n",
    "        \n",
    "    print(f\"[KNOWLEDGE] Retrieved and processed {successful_urls} out of {len(url_results)} URLs\")\n",
    "    return [{\"status\": \"knowledge search completed\", \"total_urls\": len(url_results)}]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knowledge_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when knowledge has been retrieved and pass a summary back to it.\"),\n",
    "    retrieve_web_knowledge_tool\n",
    "]\n",
    "\n",
    "knowledge_agent_node = create_react_agent(\n",
    "    model,\n",
    "    knowledge_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "        You are a Knowledge Retrieval Agent tasked with extracting accurate, up-to-date legal information from official U.S. government sources. You use the retrieve_web_knowledge_tool to search for legal/policy topics, scrape and clean page content, generate embeddings, store them in Qdrant, and retrieve top relevant results. \n",
    "        Your work is crucial for ensuring that the Compliance Checker Agent has access to the most current and relevant legal information.\n",
    "\n",
    "        Use the retrieve_web_knowledge_tool to fetch up-to-date legal data from trusted government sites and perform the following functions:\n",
    "        * Retrieve relevant statutes, regulations, and policies based on a given topic.\n",
    "        * Ensure content is current, authoritative, and clearly summarized.\n",
    "        * Avoid non-official, outdated, or speculative sources.\n",
    "        * Store the retrieved knowledge in a vector database for later use by the Compliance Checker Agent.\n",
    "\n",
    "        Search Query Format:\n",
    "        site:[gov source] \"[TOPIC]\" AND \"[FOCUS]\" AND (\"[KEYWORD1]\" OR \"[KEYWORD2]\") after:[YEAR]\n",
    "\n",
    "        Sources: congress.gov, govinfo.gov, law.cornell.edu, federalregister.gov, ecfr.gov, justice.gov, whitehouse.gov\n",
    "\n",
    "        Output Format: Return a simple sentence with the status of the knowledge retrieval.\n",
    "\n",
    "        Guidelines:\n",
    "        * Use only listed government sources.\n",
    "        * Do not fabricate or paraphrase inaccurately.\n",
    "        * If no reliable info is found, say so.\n",
    "        * [CRITICAL STEP] Return to the Planner Agent to update that the knowledge retrieval is completed.\n",
    "    \"\"\",\n",
    "    name=\"Knowledge Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from agents.compliance_checker import check_legal_compliance\n",
    "from context_bank import ContextBank\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from agents.utils.websearcher import WebContentRetriever\n",
    "\n",
    "# Create an instance of WebContentRetriever to use for querying the vector database\n",
    "def get_knowledge_from_vector_db(query, jurisdiction, document_type: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieves legal knowledge from the vector database based on the query,\n",
    "    jurisdiction, and knowledge type.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string\n",
    "        jurisdiction: The legal jurisdiction (default: \"US\")\n",
    "        \n",
    "    Returns:\n",
    "        List of relevant knowledge items with title, content, URL, and relevance score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Refine the query with jurisdiction and knowledge type for better results\n",
    "    refined_query = f\"{query} jurisdiction:{jurisdiction} document_type:{document_type}\"\n",
    "    print(f\"[KNOWLEDGE RETRIEVAL] Refined query: {refined_query}\")\n",
    "\n",
    "    try:\n",
    "        print(\"[KNOWLEDGE RETRIEVAL] Querying vector database...\")\n",
    "        query_vec = _embeddings_model.embed_query(text=refined_query)\n",
    "        search_results = _qdrant_client.search(\n",
    "            collection_name=_qdrant_collection,\n",
    "            query_vector=query_vec,\n",
    "            with_payload=True,\n",
    "            limit=_num_results\n",
    "        ) # search returns Hit objects directly\n",
    "\n",
    "        print(f\"[KNOWLEDGE RETRIEVAL] Retrieved {len(search_results)} search results\")\n",
    "\n",
    "        results = [\n",
    "            {\n",
    "                \"title\": result.payload[\"title\"],\n",
    "                \"content\": result.payload[\"content\"][:400] + \"...\",\n",
    "                \"url\": result.payload[\"url\"],\n",
    "                \"relevance\": result.score\n",
    "            }\n",
    "            for result in search_results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to retrieve knowledge from vector DB: {e}\")\n",
    "        return [] # Return empty list on search failure\n",
    "    \n",
    "    print(f\"[KNOWLEDGE RETRIEVAL] Completed with {len(results)} knowledge items\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Create a tool that the ComplianceCheckerAgent can use\n",
    "# TODO : Figure out the parameters for the compliance check tool and update the function signature and description\n",
    "@tool\n",
    "def compliance_check_tool() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Tool for checking legal document clauses for compliance issues.\n",
    "    Performs the following tasks:\n",
    "    Fetches all legal clauses from the context bank\n",
    "    Retrieves relevant legal laws and regulations from the vector database\n",
    "    Performs compliance checks on the clauses using the gathered knowledge\n",
    "    Detects compliance issues: statutory, precedent-based, internal.\n",
    "    Ensures internal consistency across clauses\n",
    "    Identifies legal risks and their implications\n",
    "    Provides structured legal reasoning and confidence scores\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        List of non-compliant clauses with detailed analysis\n",
    "    \"\"\"\n",
    "\n",
    "    # Adding a delay here to avoid hitting API rate limits\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Get clauses from context bank\n",
    "    clauses = context_bank.get_clauses()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"[COMPLIANCE CHECKER] Starting compliance check for {len(clauses)} clauses\")\n",
    "    # print(f\"[COMPLIANCE CHECKER] Context Bank state at start: {json.dumps(context_bank.get_all(), indent=2)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(f\"The list of clauses is: {clauses}\")\n",
    "\n",
    "\n",
    "    query = \"Retrieve all relevant legal knowledge for compliance checking.\"\n",
    "\n",
    "    jurisdiction = context_bank.get_jurisdiction()\n",
    "  \n",
    "    doc_meta_from_bank = context_bank.get_document()\n",
    "    # Extract the document_type from the retrieved metadata\n",
    "    # Provide a default value if the key is missing\n",
    "    document_type = doc_meta_from_bank.get(\"document_type\", \"Unknown Document Type\") \n",
    "    \n",
    "    print(f\"[COMPLIANCE CHECKER] Using jurisdiction: {jurisdiction}\")\n",
    "    print(f\"[COMPLIANCE CHECKER] Using document type: {document_type}\")\n",
    "\n",
    "    # Create a knowledge retrieval adapter that mimics a knowledge agent\n",
    "    # but actually uses the vector DB directly\n",
    "    print(\"[COMPLIANCE CHECKER] Retrieving knowledge from vector DB...\")\n",
    "    knowledge_data = get_knowledge_from_vector_db(query, jurisdiction, document_type)\n",
    "    print(f\"[COMPLIANCE CHECKER] Retrieved {len(knowledge_data)} knowledge items\")\n",
    "    \n",
    "    print(\"[COMPLIANCE CHECKER] Checking legal compliance...\")\n",
    "    results = check_legal_compliance(\n",
    "        context_bank=context_bank,\n",
    "        knowledge_from_vector_db=knowledge_data,\n",
    "        use_ollama=False,\n",
    "        # model_name=\"llama3.1:latest\",\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        min_confidence=0.75\n",
    "    )\n",
    "    \n",
    "    print(f\"[COMPLIANCE CHECKER] Compliance check completed with {len(results)} results\")\n",
    "    # print(\"\\n\" + \"=\"*80)\n",
    "    # print(f\"[COMPLIANCE CHECKER] Context Bank state after check: {json.dumps(context_bank.get_all(), indent=2)}\")\n",
    "    # print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compliance_checker_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Knowledge Agent\", description=\"Transfer to Knowledge Agent if more knowledge is needed, it helps to retrieve knowledge from the web using websearcher.\"),\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when compliance checking is completed and all clauses are found to be compliant, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "    compliance_check_tool\n",
    "]\n",
    "\n",
    "compliance_checker_agent_node = create_react_agent(\n",
    "    model,\n",
    "    compliance_checker_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "      You are the Compliance Checker Agent, responsible for analyzing a list of extracted legal clauses to identify contradictions, ensure statutory compliance, and assess contractual consistency under U.S. law.\n",
    "      You will use the compliance_check_tool to fetch all the **legal clauses from the context bank** and **relevant laws and regulations retrieved from the vector database** to perform a compliance check for the clauses against the legal knowledge.\n",
    "      Your task is to ensure that the clauses are compliant with federal, state, and city laws, and to identify any legal risks or implications associated with non-compliance.\n",
    "      Your work is crucial for ensuring that the legal document is compliant with all relevant laws and regulations.\n",
    "\n",
    "      Use the compliance_check_tool to perform the following primary functions:\n",
    "      * Fetch all legal clauses from the context bank\n",
    "      * Retrieve relevant legal laws and regulations from the vector database\n",
    "      * Perform compliance checks on the clauses using the gathered knowledge\n",
    "      * Detect compliance issues: statutory, precedent-based, internal.\n",
    "      * Ensure internal consistency across clauses\n",
    "      * Identify legal risks and their implications\n",
    "      * Provide structured legal reasoning and confidence scores\n",
    "\n",
    "      Output:\n",
    "      1. Contradiction Report\n",
    "      {\n",
    "        \"has_contradiction\": true|false,\n",
    "        \"contradiction_type\": \"statutory|precedent|internal\",\n",
    "        \"severity\": \"high|medium|low\",\n",
    "        \"description\": \"...\",\n",
    "        \"source_clause\": { \"id\": \"...\", \"text\": \"...\" },\n",
    "        \"reference\": { \"type\": \"...\", \"id\": \"...\", \"text\": \"...\" }\n",
    "      }\n",
    "      2. Reasoning & Analysis\n",
    "      {\n",
    "        \"analysis_steps\": [\"Step 1...\", \"Step 2...\", \"Step 3...\"],\n",
    "        \"confidence_score\": 0.0–1.0,\n",
    "        \"supporting_references\": [{ \"type\": \"statute\", \"id\": \"...\", \"relevance\": \"...\" }]\n",
    "      }\n",
    "      3. Legal Implications\n",
    "      {\n",
    "        \"implications\": [\n",
    "          {\n",
    "            \"description\": \"...\",\n",
    "            \"severity\": \"high|medium|low\",\n",
    "            \"affected_parties\": [\"...\"],\n",
    "            \"risk_areas\": [\"...\"]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "\n",
    "\n",
    "      [CRITICAL STEP] Decision Flow:\n",
    "      If information is deemed insufficient, call the Knowledge Agent with correct field to retrieve information on the missing topic\n",
    "      If compliance check is complete, return to the Planner Agent for post processing where the whole process is summarized by the Post Processor Agent.\n",
    "\n",
    "      Guidelines:\n",
    "      * Use only validated legal sources\n",
    "      * No fabrication or assumptions\n",
    "      * Flag unclear issues and recommend human review when needed\n",
    "      * Consider jurisdictional scope and maintain objectivity\n",
    "      * [CRITICAL STEP] Return to the Planner Agent to update that the compliance check is completed.\n",
    "    \"\"\",\n",
    "    name=\"Compliance Checker Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO : Add the other tools for Clause Rewriter Agent -\n",
    "\n",
    "# clause_rewriter_agent_tools = [\n",
    "#     create_handoff_tool(agent_name=\"Compliance Checker Agent\", description=\"Transfer to Compliance Checker Agent after a non-compliant clause has been rewritten, it helps to check the compliance of the rewritten clause.\"),\n",
    "# ]\n",
    "\n",
    "# clause_rewriter_agent_node = create_react_agent(\n",
    "#     model,\n",
    "#     clause_rewriter_agent_tools,\n",
    "#     prompt=\"\"\"\n",
    "#     You are the Clause Rewriter Agent, tasked with revising legal clauses flagged as non-compliant, contradictory, or unclear by the Compliance Checker Agent. Your goal is to ensure legal compliance while preserving the original intent.\n",
    "\n",
    "# Responsibilities:\n",
    "# Rewrite clauses to resolve statutory, precedent-based, or internal contradictions\n",
    "# Ensure clarity, enforceability, and alignment with U.S. law\n",
    "# Maintain intent and context of original clause\n",
    "# Signal if more legal context is required (route to Knowledge Agent)\n",
    "\n",
    "# Input Format:\n",
    "# {\n",
    "#   \"original_clause\": {\n",
    "#     \"id\": \"clause_id\",\n",
    "#     \"text\": \"original text\"\n",
    "#   },\n",
    "#   \"issue\": {\n",
    "#     \"description\": \"reason for non-compliance\",\n",
    "#     \"contradiction_type\": \"statutory|precedent|internal\",\n",
    "#     \"reference\": {\n",
    "#       \"type\": \"statute|precedent|clause\",\n",
    "#       \"text\": \"reference text\",\n",
    "#       \"source_link\": \"optional\"\n",
    "#     }\n",
    "#   },\n",
    "#   \"context_info\": {\n",
    "#     \"document_title\": \"Title\",\n",
    "#     \"named_entities\": [...],\n",
    "#     \"document_class\": \"e.g., NDA, Lease\"\n",
    "#   }\n",
    "# }\n",
    "# Output Format:\n",
    "# {\n",
    "#   \"clause_id\": \"clause_id\",\n",
    "#   \"rewritten_clause\": \"Compliant version of the clause\",\n",
    "#   \"justification\": \"How it resolves the issue and aligns with legal standards\"\n",
    "# }\n",
    "# Guidelines:\n",
    "# Be concise, precise, and legally sound\n",
    "# Do not fabricate or generalize\n",
    "# Flag insufficient context when needed\n",
    "\n",
    "#     \"\"\",\n",
    "#     name=\"Clause Rewriter Agent\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Add the other tools for Post-Processor Agent - Process Summarizer, Context Bank getter\n",
    "\n",
    "post_processor_agent_tools = [\n",
    "    create_handoff_tool(agent_name=\"Planner Agent\", description=\"Transfer to Planner Agent when post-processing is completed, it helps to plan the next steps in the workflow and delegate tasks.\"),\n",
    "]\n",
    "\n",
    "post_processor_agent_node = create_react_agent(\n",
    "    model,\n",
    "    post_processor_agent_tools,\n",
    "    prompt=\"\"\"\n",
    "        You are the Post-processor Agent, responsible for generating final, human-readable outputs after a legal document passes all compliance checks.\n",
    "\n",
    "        Input:\n",
    "        Context Bank (document metadata, clause info)\n",
    "        Compliance Checker outputs (reasoning, implications)\n",
    "        History of tasks done\n",
    "        Tools: Summarizer\n",
    "        Outputs (via Process Summarizer):\n",
    "        Contract Summary – Overview of the document\n",
    "        Changes – Highlighted clause modifications\n",
    "        Risks Averted – Legal issues resolved\n",
    "        References – Cited statutes and precedents\n",
    "\n",
    "        Guidelines:\n",
    "        * Be clear, concise, and legally accurate\n",
    "        * Avoid jargon or speculation\n",
    "        * Tailor for legal and business audiences\n",
    "        * [CRITICAL STEP] Return the summary to the Planner Agent and update that the post processing is completed.\n",
    "\"\"\",\n",
    "    name=\"Post Processor Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[WORKFLOW] Multi-agent swarm initialized with the following agents:\n",
      "  - Planner Agent\n",
      "  - Pre Processor Agent\n",
      "  - Knowledge Agent\n",
      "  - Compliance Checker Agent\n",
      "  - Post Processor Agent\n",
      "[WORKFLOW] Default active agent: Planner Agent\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpointer = InMemorySaver()\n",
    "workflow = create_swarm(\n",
    "    [planner_agent_node, pre_processor_agent_node, knowledge_agent_node, compliance_checker_agent_node, post_processor_agent_node],\n",
    "    default_active_agent=\"Planner Agent\"\n",
    ")\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[WORKFLOW] Multi-agent swarm initialized with the following agents:\")\n",
    "print(\"  - Planner Agent\")\n",
    "print(\"  - Pre Processor Agent\")\n",
    "print(\"  - Knowledge Agent\")\n",
    "print(\"  - Compliance Checker Agent\")\n",
    "print(\"  - Post Processor Agent\")\n",
    "print(\"[WORKFLOW] Default active agent: Planner Agent\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[PREPROCESS] Starting document preprocessing for: ./Original and Modified/modified_UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2.pdf\n",
      "================================================================================\n",
      "\n",
      "[PREPROCESS] Step 1: Extracting text from PDF...\n",
      "[PREPROCESS] Extracted 167916 characters of text\n",
      "[PREPROCESS] Step 2: Extracting title...\n",
      "[PREPROCESS] Extracted title: Based on the text provided, the title of the document is:\n",
      "\n",
      "**NETWORK 1 FINANCIAL CORPORATION AFFILIATE OFFICE AGREEMENT**\n",
      "[PREPROCESS] Step 3: Performing Named Entity Recognition...\n",
      "[PREPROCESS] Extracted 3165 named entities\n",
      "[PREPROCESS] Sample entities (first 5): [('10.11', 'CARDINAL'), ('NETWORK 1 FINANCIAL CORPORATION', 'WORK_OF_ART'), ('AGREEMENT', 'GPE'), ('NETWORK 1 FINANCIAL, INC.', 'ORG'), ('NETWORK 1', 'WORK_OF_ART')]\n",
      "[PREPROCESS] Step 4: Classifying document and clauses...\n",
      "[PREPROCESS] Successfully parsed LLM output as JSON\n",
      "[PREPROCESS] Document class: Legal Agreement - Affiliate Office Agreement\n",
      "[PREPROCESS] Extracted 6 clauses\n",
      "[PREPROCESS] Step 5: Storing in Context Bank...\n",
      "[PREPROCESS] Data successfully stored in Context Bank\n",
      "[KNOWLEDGE] Searching Tavily for query: site:govinfo.gov \"Affiliate Office Agreement\" AND \"Legal Requirements\" AND (\"Compliance\" OR \"Regulations\") after:2020\n",
      "[KNOWLEDGE] Found 10 results from Tavily search\n",
      "[KNOWLEDGE] Processing result 1/10: Federal Register/Vol. 89, No. 12/Thursday, January 18, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2024-01-18/pdf/2023-28629.pdf\n",
      "[KNOWLEDGE] Processing result 2/10: Federal Register/Vol. 89, No. 49/Tuesday, March 12, 2024/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2024-03-12/pdf/2024-05207.pdf\n",
      "[KNOWLEDGE] Processing result 3/10: Rules and Regulations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2021-02-23/pdf/2020-28473.pdf\n",
      "[KNOWLEDGE] Processing result 4/10: Rules and Regulations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2021-02-12/pdf/2021-01499.pdf\n",
      "[KNOWLEDGE] Processing result 5/10: Rules and Regulations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2022-01-28/pdf/2022-01607.pdf\n",
      "[KNOWLEDGE] Processing result 6/10: Federal Register/Vol. 89, No. 78/Monday, April 22, 2024/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2024-04-22/pdf/2024-07496.pdf\n",
      "[KNOWLEDGE] Processing result 7/10: Federal Register/Vol. 90, No. 5/Wednesday, January 8, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2025-01-08/pdf/2024-31486.pdf\n",
      "[KNOWLEDGE] Processing result 8/10: Federal Register/Vol. 89, No. 154/Friday, August 9, 2024/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2024-08-09/pdf/2024-17351.pdf\n",
      "[KNOWLEDGE] Processing result 9/10: Public Law 95-369 95th Congress An Act\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] Successfully processed URL: https://www.govinfo.gov/content/pkg/STATUTE-92/pdf/STATUTE-92-Pg607.pdf\n",
      "[KNOWLEDGE] Processing result 10/10: Statement of Policy on Bank Merger Transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE] No content found or extracted for URL https://www.govinfo.gov/content/pkg/FR-2024-09-27/pdf/2024-22189.pdf\n",
      "[KNOWLEDGE] Upserting 1 documents to Qdrant collection\n",
      "[KNOWLEDGE] Successfully upserted 1 documents to Qdrant\n",
      "[KNOWLEDGE] Retrieved and processed 1 out of 10 URLs\n",
      "\n",
      "================================================================================\n",
      "[COMPLIANCE CHECKER] Starting compliance check for 6 clauses\n",
      "================================================================================\n",
      "\n",
      "The list of clauses is: [{'Text': 'THIS AGREEMENT is entered into by and between NETWORK 1 FINANCIAL, INC. (\"NETWORK 1\"), a Virginia Corporation with its principal place of business at 1501 Farm Credit Drive, Suite 1500, McLean, Virginia 22102-5004, and Payment Data Systems, Inc., the Affiliate Office (\"AFFILIATE\"), a Nevada Corporation with its principal place of business at 12500 San Pedro Suite 120 San Antonio, TX 78216.', 'Category': 'Parties and Definitions'}, {'Text': 'The term (\"Term\") of this Agreement shall be for one hundred eighty days (180) from the date set forth below unless Network 1 or Visa or MasterCard or Harris Bank doesn\\'t approve Affiliate\\'s ISO application, in which case, the Term will be 3 years. This Agreement will automatically renew for successive one-year terms unless terminated by either party by providing the other with 30 days written notice that this Agreement will not be renewed or Affiliate enters into a Processing agreement with Network 1 and an ISO Sponsorship agreement with Harris Bank in which case this Agreement will automatically terminate concurrent with the execution of such agreements.', 'Category': 'Term and Renewal'}, {'Text': 'Agreement may be terminated prior to the conclusion of the Term by giving written notice of termination: A. By either party as a result of default by the other party under this Agreement and failure to cure said default within thirty (30) days after notice of said default is given.', 'Category': 'Termination Clause'}, {'Text': \"Affiliate hereby agrees to indemnify and hold harmless Network 1, VISA, MasterCard and the Member Bank from and against any loss, cost or damage (including reasonable legal fees and court costs) incurred by Network 1, VISA, MasterCard and the Member Bank as a result of Affiliate's failure to comply with the terms of this Agreement, Affiliate's misrepresentation with respect to this Agreement or Affiliate's knowing or negligent misrepresentation with respect to Contractors.\", 'Category': 'Indemnification Clause'}, {'Text': 'All disputes or claims hereunder shall be resolved by arbitration in McLean, Virginia, pursuant to the rules of the American Arbitration Association.', 'Category': 'Arbitration/Dispute Resolution'}, {'Text': 'This agreement may be assigned or delegated, in whole or in part, by NETWORK 1 without the prior written consent of the other party herein. This agreement may not be assigned or delegated by Affiliate without prior written consent from Network 1. Such consent shall not be unreasonably withheld.', 'Category': 'Assignment'}]\n",
      "[COMPLIANCE CHECKER] Using jurisdiction: None\n",
      "[COMPLIANCE CHECKER] Using document type: Unknown Document Type\n",
      "[COMPLIANCE CHECKER] Retrieving knowledge from vector DB...\n",
      "[KNOWLEDGE RETRIEVAL] Refined query: Retrieve all relevant legal knowledge for compliance checking. jurisdiction:None document_type:Unknown Document Type\n",
      "[KNOWLEDGE RETRIEVAL] Querying vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/67/79fgxv_911q4lk20g_fbhp7h0000gn/T/ipykernel_51360/3379753579.py:31: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = _qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNOWLEDGE RETRIEVAL] Retrieved 3 search results\n",
      "[KNOWLEDGE RETRIEVAL] Completed with 3 knowledge items\n",
      "[COMPLIANCE CHECKER] Retrieved 3 knowledge items\n",
      "[COMPLIANCE CHECKER] Checking legal compliance...\n",
      "Starting legal compliance check with model 'gemini-2.0-flash' (use_ollama=False)\n",
      "Initializing API client with model gemini-2.0-flash\n",
      "WARNING: No jurisdiction found in context bank, estimating from document content\n",
      "Estimated jurisdiction: Virginia\n",
      "Found 6 clauses and 3165 entities\n",
      "Processing 3 knowledge items from vector database\n",
      "Classified knowledge: 2 statutes, 1 precedents\n",
      "Prepared knowledge context with 2 statutes and 1 precedents\n",
      "Analyzing clause 1/6 (ID: unknown)\n",
      "Clause Text: THIS AGREEMENT is entered into by and between NETWORK 1 FINANCIAL, INC. (\"NETWORK 1\"), a Virginia Corporation with its principal place of business at 1501 Farm Credit Drive, Suite 1500, McLean, Virginia 22102-5004, and Payment Data Systems, Inc., the Affiliate Office (\"AFFILIATE\"), a Nevada Corporation with its principal place of business at 12500 San Pedro Suite 120 San Antonio, TX 78216.\n",
      "Clause Category: Parties and Definitions\n",
      "Analyzing compliance for clause ID ff4c751c-d900-429f-8f32-7423f2dd3e5d\n",
      "Checking statutory compliance against 2 statutes\n",
      "Validating statutory compliance for clause ff4c751c-d900-429f-8f32-7423f2dd3e5d in Virginia jurisdiction\n",
      "Analyzing against 2 relevant statutes\n",
      "ERROR: Failed to generate statutory analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 statutory violations\n",
      "Checking precedent compliance against 1 precedents\n",
      "Analyzing precedent compliance for clause ff4c751c-d900-429f-8f32-7423f2dd3e5d\n",
      "Analyzing against 1 precedents in Virginia jurisdiction\n",
      "ERROR: Failed to generate precedent analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 precedent issues\n",
      "Checking consistency against 0 other clauses\n",
      "Starting contractual consistency check for 1 clauses (min_confidence=0.75)\n",
      "WARNING: Not enough clauses for consistency analysis\n",
      "Found 0 consistency issues\n",
      "Analyzing clause 2/6 (ID: unknown)\n",
      "Clause Text: The term (\"Term\") of this Agreement shall be for one hundred eighty days (180) from the date set forth below unless Network 1 or Visa or MasterCard or Harris Bank doesn't approve Affiliate's ISO application, in which case, the Term will be 3 years. This Agreement will automatically renew for successive one-year terms unless terminated by either party by providing the other with 30 days written notice that this Agreement will not be renewed or Affiliate enters into a Processing agreement with Network 1 and an ISO Sponsorship agreement with Harris Bank in which case this Agreement will automatically terminate concurrent with the execution of such agreements.\n",
      "Clause Category: Term and Renewal\n",
      "Analyzing compliance for clause ID 56619c13-52d5-4a5a-9753-9cb16e4e9048\n",
      "Checking statutory compliance against 2 statutes\n",
      "Validating statutory compliance for clause 56619c13-52d5-4a5a-9753-9cb16e4e9048 in Virginia jurisdiction\n",
      "Analyzing against 2 relevant statutes\n",
      "ERROR: Failed to generate statutory analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 statutory violations\n",
      "Checking precedent compliance against 1 precedents\n",
      "Analyzing precedent compliance for clause 56619c13-52d5-4a5a-9753-9cb16e4e9048\n",
      "Analyzing against 1 precedents in Virginia jurisdiction\n",
      "ERROR: Failed to generate precedent analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 precedent issues\n",
      "Checking consistency against 0 other clauses\n",
      "Starting contractual consistency check for 1 clauses (min_confidence=0.75)\n",
      "WARNING: Not enough clauses for consistency analysis\n",
      "Found 0 consistency issues\n",
      "Analyzing clause 3/6 (ID: unknown)\n",
      "Clause Text: Agreement may be terminated prior to the conclusion of the Term by giving written notice of termination: A. By either party as a result of default by the other party under this Agreement and failure to cure said default within thirty (30) days after notice of said default is given.\n",
      "Clause Category: Termination Clause\n",
      "Analyzing compliance for clause ID 8ab4ffe0-0089-4c94-89e8-eb58e63cc33d\n",
      "Checking statutory compliance against 2 statutes\n",
      "Validating statutory compliance for clause 8ab4ffe0-0089-4c94-89e8-eb58e63cc33d in Virginia jurisdiction\n",
      "Analyzing against 2 relevant statutes\n",
      "ERROR: Failed to generate statutory analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 statutory violations\n",
      "Checking precedent compliance against 1 precedents\n",
      "Analyzing precedent compliance for clause 8ab4ffe0-0089-4c94-89e8-eb58e63cc33d\n",
      "Analyzing against 1 precedents in Virginia jurisdiction\n",
      "ERROR: Failed to generate precedent analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 precedent issues\n",
      "Checking consistency against 0 other clauses\n",
      "Starting contractual consistency check for 1 clauses (min_confidence=0.75)\n",
      "WARNING: Not enough clauses for consistency analysis\n",
      "Found 0 consistency issues\n",
      "Analyzing clause 4/6 (ID: unknown)\n",
      "Clause Text: Affiliate hereby agrees to indemnify and hold harmless Network 1, VISA, MasterCard and the Member Bank from and against any loss, cost or damage (including reasonable legal fees and court costs) incurred by Network 1, VISA, MasterCard and the Member Bank as a result of Affiliate's failure to comply with the terms of this Agreement, Affiliate's misrepresentation with respect to this Agreement or Affiliate's knowing or negligent misrepresentation with respect to Contractors.\n",
      "Clause Category: Indemnification Clause\n",
      "Analyzing compliance for clause ID ba058dd6-ae9f-43c2-82d5-079522d421b4\n",
      "Checking statutory compliance against 2 statutes\n",
      "Validating statutory compliance for clause ba058dd6-ae9f-43c2-82d5-079522d421b4 in Virginia jurisdiction\n",
      "Analyzing against 2 relevant statutes\n",
      "ERROR: Failed to generate statutory analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 statutory violations\n",
      "Checking precedent compliance against 1 precedents\n",
      "Analyzing precedent compliance for clause ba058dd6-ae9f-43c2-82d5-079522d421b4\n",
      "Analyzing against 1 precedents in Virginia jurisdiction\n",
      "ERROR: Failed to generate precedent analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 precedent issues\n",
      "Checking consistency against 0 other clauses\n",
      "Starting contractual consistency check for 1 clauses (min_confidence=0.75)\n",
      "WARNING: Not enough clauses for consistency analysis\n",
      "Found 0 consistency issues\n",
      "Analyzing clause 5/6 (ID: unknown)\n",
      "Clause Text: All disputes or claims hereunder shall be resolved by arbitration in McLean, Virginia, pursuant to the rules of the American Arbitration Association.\n",
      "Clause Category: Arbitration/Dispute Resolution\n",
      "Analyzing compliance for clause ID 04197536-facd-4b17-9d0f-8b6d3352b0ea\n",
      "Checking statutory compliance against 2 statutes\n",
      "Validating statutory compliance for clause 04197536-facd-4b17-9d0f-8b6d3352b0ea in Virginia jurisdiction\n",
      "Analyzing against 2 relevant statutes\n",
      "ERROR: Failed to generate statutory analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 statutory violations\n",
      "Checking precedent compliance against 1 precedents\n",
      "Analyzing precedent compliance for clause 04197536-facd-4b17-9d0f-8b6d3352b0ea\n",
      "Analyzing against 1 precedents in Virginia jurisdiction\n",
      "ERROR: Failed to generate precedent analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 precedent issues\n",
      "Checking consistency against 0 other clauses\n",
      "Starting contractual consistency check for 1 clauses (min_confidence=0.75)\n",
      "WARNING: Not enough clauses for consistency analysis\n",
      "Found 0 consistency issues\n",
      "Analyzing clause 6/6 (ID: unknown)\n",
      "Clause Text: This agreement may be assigned or delegated, in whole or in part, by NETWORK 1 without the prior written consent of the other party herein. This agreement may not be assigned or delegated by Affiliate without prior written consent from Network 1. Such consent shall not be unreasonably withheld.\n",
      "Clause Category: Assignment\n",
      "Analyzing compliance for clause ID 30568740-4a0e-422c-a8a7-79264f0232fe\n",
      "Checking statutory compliance against 2 statutes\n",
      "Validating statutory compliance for clause 30568740-4a0e-422c-a8a7-79264f0232fe in Virginia jurisdiction\n",
      "Analyzing against 2 relevant statutes\n",
      "ERROR: Failed to generate statutory analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 statutory violations\n",
      "Checking precedent compliance against 1 precedents\n",
      "Analyzing precedent compliance for clause 30568740-4a0e-422c-a8a7-79264f0232fe\n",
      "Analyzing against 1 precedents in Virginia jurisdiction\n",
      "ERROR: Failed to generate precedent analysis: 'APIClient' object has no attribute 'generate'\n",
      "Found 0 precedent issues\n",
      "Checking consistency against 0 other clauses\n",
      "Starting contractual consistency check for 1 clauses (min_confidence=0.75)\n",
      "WARNING: Not enough clauses for consistency analysis\n",
      "Found 0 consistency issues\n",
      "Stored document analysis in context bank: 0 non-compliant clauses with 0 issues\n",
      "Completed compliance check in 197.15 seconds. Found 0 non-compliant clauses\n",
      "[COMPLIANCE CHECKER] Compliance check completed with 0 results\n",
      "{'messages': [HumanMessage(content='You are given a file path the document which you must preprocess to extract clauses. Once the clauses are extracted, fetch all relevant knowledge related to it. Based on the collected knowledge, you should check for compliance of these clauses. Explain the non-compliant clauses, suggest changes and summarize the results for the User. FILE PATH OF DOCUMENT: \"./Original and Modified/modified_UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2.pdf\" \",\\n            ', additional_kwargs={}, response_metadata={}, id='453d558c-3804-4684-beb8-ab31f6b83de2'), AIMessage(content='Okay, I understand the problem. I need to analyze the legal document at \"./Original and Modified/modified_UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2.pdf\" for discrepancies and compliance issues. Here\\'s my plan:\\n\\n1.  **Status Check:** I will first check the status of the Preprocessor, Compliance Checker, and Post-Processor agents to avoid redundant computations.\\n2.  **Preprocess Document:** If the Preprocessor agent hasn\\'t completed its task, I\\'ll trigger it to preprocess the document, extract clauses, and classify them.\\n3.  **Retrieve Relevant Knowledge:** Once preprocessing is complete, I\\'ll trigger the Knowledge Agent to retrieve relevant legal compliance knowledge from the web.\\n4.  **Check Clause Compliance:** After the Knowledge Agent retrieves the knowledge, I\\'ll trigger the Compliance Checker Agent to check the compliance of the clauses with legal regulations.\\n5.  **Summarize Issues and Finalize Output:** Finally, after the compliance check is complete, I\\'ll trigger the Post Processor Agent to summarize the issues and finalize the output for the user.\\n\\nLet\\'s start by triggering the Preprocessor Agent since this is the first step.', additional_kwargs={'function_call': {'name': 'transfer_to_pre_processor_agent', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Planner Agent', id='run-8f64f385-df18-4899-b686-801ad2eaf66f-0', tool_calls=[{'name': 'transfer_to_pre_processor_agent', 'args': {}, 'id': 'dbf6b255-eda8-4930-b73f-03efef8d9c13', 'type': 'tool_call'}], usage_metadata={'input_tokens': 678, 'output_tokens': 289, 'total_tokens': 967, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Successfully transferred to Pre Processor Agent', name='transfer_to_pre_processor_agent', id='e2adfe02-7907-41a1-8047-1d4f1dd40e09', tool_call_id='dbf6b255-eda8-4930-b73f-03efef8d9c13'), AIMessage(content='Okay, I will now preprocess the document at the file path \"./Original and Modified/modified_UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2.pdf\" to extract the full text, classify the document, identify named entities, and extract key clauses. After extracting this information, I will store it as a JSON object.', additional_kwargs={'function_call': {'name': 'preprocess_document_tool', 'arguments': '{\"file_path\": \"./Original and Modified/modified_UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2.pdf\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Pre Processor Agent', id='run-71be76c3-d3a1-402f-b8e1-db59aa214ef0-0', tool_calls=[{'name': 'preprocess_document_tool', 'args': {'file_path': './Original and Modified/modified_UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2.pdf'}, 'id': '720e5e41-0780-4d72-80a5-6648acdbc95c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 530, 'output_tokens': 163, 'total_tokens': 693, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='{\"Document Title\": \"Based on the text provided, the title of the document is:\\\\n\\\\n**NETWORK 1 FINANCIAL CORPORATION AFFILIATE OFFICE AGREEMENT**\", \"Document Class\": \"Legal Agreement - Affiliate Office Agreement\"}', name='preprocess_document_tool', id='f76de03a-f5a4-47bb-ab30-229ed7c6a8c3', tool_call_id='720e5e41-0780-4d72-80a5-6648acdbc95c'), AIMessage(content='OK. I have preprocessed the document. Now I need to signal to the Planner Agent that I am done.', additional_kwargs={'function_call': {'name': 'transfer_to_planner_agent', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Pre Processor Agent', id='run-2b073928-04af-4f70-913c-5fffc1395284-0', tool_calls=[{'name': 'transfer_to_planner_agent', 'args': {}, 'id': 'fa206e1a-aa84-4e91-9476-1cba474edf0c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 633, 'output_tokens': 31, 'total_tokens': 664, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Successfully transferred to Planner Agent', name='transfer_to_planner_agent', id='25a401b6-d0d4-40b5-95de-5b4a8e45be81', tool_call_id='fa206e1a-aa84-4e91-9476-1cba474edf0c'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'transfer_to_knowledge_agent', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Planner Agent', id='run-46cd1691-a51a-4d5e-89d9-fef0a0061279-0', tool_calls=[{'name': 'transfer_to_knowledge_agent', 'args': {}, 'id': '01f201e2-e129-46db-abbd-f0d8c37832e1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 826, 'output_tokens': 7, 'total_tokens': 833, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Successfully transferred to Knowledge Agent', name='transfer_to_knowledge_agent', id='067995f0-43bc-4561-a37f-ec69ca39f5e1', tool_call_id='01f201e2-e129-46db-abbd-f0d8c37832e1'), AIMessage(content='Okay, I will start by retrieving knowledge related to \"Affiliate Office Agreements\" from government sources.', additional_kwargs={'function_call': {'name': 'retrieve_web_knowledge_tool', 'arguments': '{\"query\": \"site:govinfo.gov \\\\\"Affiliate Office Agreement\\\\\" AND \\\\\"Legal Requirements\\\\\" AND (\\\\\"Compliance\\\\\" OR \\\\\"Regulations\\\\\") after:2020\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Knowledge Agent', id='run-ccf5cff7-97b4-460a-b40f-73bda7dc1324-0', tool_calls=[{'name': 'retrieve_web_knowledge_tool', 'args': {'query': 'site:govinfo.gov \"Affiliate Office Agreement\" AND \"Legal Requirements\" AND (\"Compliance\" OR \"Regulations\") after:2020'}, 'id': '7481873b-d676-433b-9048-8d9598773af0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 718, 'output_tokens': 59, 'total_tokens': 777, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='[{\"status\": \"knowledge search completed\", \"total_urls\": 10}]', name='retrieve_web_knowledge_tool', id='6fbcf22c-77b0-4663-ac2d-1d33adf1a9ee', tool_call_id='7481873b-d676-433b-9048-8d9598773af0'), AIMessage(content='Knowledge retrieval is completed.', additional_kwargs={'function_call': {'name': 'transfer_to_planner_agent', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Knowledge Agent', id='run-8679fcf6-f00e-47c6-a405-43f89d43e123-0', tool_calls=[{'name': 'transfer_to_planner_agent', 'args': {}, 'id': '57e95f8b-36ca-4190-a76d-b523d4dae76a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 773, 'output_tokens': 13, 'total_tokens': 786, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Successfully transferred to Planner Agent', name='transfer_to_planner_agent', id='7d36d966-910a-43f7-9ce9-6ccce740facb', tool_call_id='57e95f8b-36ca-4190-a76d-b523d4dae76a'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'transfer_to_compliance_checker_agent', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Planner Agent', id='run-a82ddd48-71d0-4866-9a83-25bd6f2512c6-0', tool_calls=[{'name': 'transfer_to_compliance_checker_agent', 'args': {}, 'id': 'fc10560b-d8b4-40b1-b7a1-2fa0f2a7d946', 'type': 'tool_call'}], usage_metadata={'input_tokens': 921, 'output_tokens': 9, 'total_tokens': 930, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Successfully transferred to Compliance Checker Agent', name='transfer_to_compliance_checker_agent', id='566dcec0-c0cd-4449-bb12-0535fda131a4', tool_call_id='fc10560b-d8b4-40b1-b7a1-2fa0f2a7d946'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'compliance_check_tool', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Compliance Checker Agent', id='run-e353a22e-39f3-4116-951c-748bce2e9aa0-0', tool_calls=[{'name': 'compliance_check_tool', 'args': {}, 'id': '216590c7-171a-4d66-bb37-646f4c2844f5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1175, 'output_tokens': 5, 'total_tokens': 1180, 'input_token_details': {'cache_read': 0}}), ToolMessage(content=[], name='compliance_check_tool', id='560814be-7a90-485c-8c34-c6fa6aea8948', tool_call_id='216590c7-171a-4d66-bb37-646f4c2844f5'), AIMessage(content='Based on the compliance check, no non-compliant clauses were detected.', additional_kwargs={'function_call': {'name': 'transfer_to_planner_agent', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Compliance Checker Agent', id='run-7a3233fb-51e2-49a8-9ecc-dae7f8fc397b-0', tool_calls=[{'name': 'transfer_to_planner_agent', 'args': {}, 'id': '12d8f7c6-4723-48b0-acb2-cbb237e39940', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1186, 'output_tokens': 22, 'total_tokens': 1208, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Successfully transferred to Planner Agent', name='transfer_to_planner_agent', id='fc178871-479c-466e-8876-cf269225ddfe', tool_call_id='12d8f7c6-4723-48b0-acb2-cbb237e39940'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'transfer_to_post_processor_agent', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Planner Agent', id='run-ab6b427f-79b9-4fcd-8fa6-03a43090ed75-0', tool_calls=[{'name': 'transfer_to_post_processor_agent', 'args': {}, 'id': '8d77a728-478d-47c1-adf8-bfce0cdc7ad6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 977, 'output_tokens': 9, 'total_tokens': 986, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Successfully transferred to Post Processor Agent', name='transfer_to_post_processor_agent', id='8dba9fde-95cd-4bb1-baf5-7c811d19f84f', tool_call_id='8d77a728-478d-47c1-adf8-bfce0cdc7ad6'), AIMessage(content='Since no compliance issues were found, I will generate a summary of the agreement.\\n\\n**Contract Summary**\\n\\nThis document is an Affiliate Office Agreement. After a thorough compliance check, no immediate legal risks were identified.\\n\\n**Changes**\\n\\nNo changes were suggested as part of the compliance check.\\n\\n**Risks Averted**\\n\\nNo specific risks were averted as no compliance issues were found.\\n\\n**References**\\n\\nNo specific statutes or precedents were referenced as no compliance issues were found.', additional_kwargs={'function_call': {'name': 'transfer_to_planner_agent', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, name='Post Processor Agent', id='run-d5b26fa1-a202-4b84-be8c-a21d526c4504-0', tool_calls=[{'name': 'transfer_to_planner_agent', 'args': {}, 'id': '8c71c223-50b7-4e1a-9835-fd68f2535fdb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 645, 'output_tokens': 105, 'total_tokens': 750, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Successfully transferred to Planner Agent', name='transfer_to_planner_agent', id='bbba7835-3c8a-45e4-9590-67e2ad6e827f', tool_call_id='8c71c223-50b7-4e1a-9835-fd68f2535fdb'), AIMessage(content='', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'MALFORMED_FUNCTION_CALL', 'safety_ratings': []}, name='Planner Agent', id='run-883b73cc-69a5-4914-8540-c76937095262-0', usage_metadata={'input_tokens': 1022, 'output_tokens': 0, 'total_tokens': 1022, 'input_token_details': {'cache_read': 0}})], 'active_agent': 'Planner Agent'}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "turn_1 = app.invoke(\n",
    "    {\"messages\": \n",
    "        [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"\"\"You are given a file path the document which you must preprocess to extract clauses. Once the clauses are extracted, fetch all relevant knowledge related to it. Based on the collected knowledge, you should check for compliance of these clauses. Explain the non-compliant clauses, suggest changes and summarize the results for the User. FILE PATH OF DOCUMENT: \\\"./Original and Modified/modified_UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2.pdf\\\" \",\n",
    "            \"\"\"\n",
    "        }]\n",
    "    },\n",
    "    config,\n",
    ")\n",
    "print(turn_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
